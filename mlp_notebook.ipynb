{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds a multi layer perceptron network using our engineered text-rule similarity features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func import embed_batch, load_or_create_embeddings, extract_text_features, combine_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_col = \"rule\"\n",
    "body_col = \"body\"  \n",
    "label_col = \"rule_violation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f24d8f46c10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device / Encoder Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2,029\n",
      "Test: 54,059\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "solution_df = pd.read_csv('data/solution.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df):,}\")\n",
    "print(f\"Test: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_name = './e5-large-v2-triplet' # custum fine tuned triplet model\n",
    "encoder_name = 'all-mpnet-base-v2' # 768-dim, better quality\n",
    "# encoder_name = 'all-MiniLM-L12-v2' # 384-dim, middle ground\n",
    "# encoder_name = 'paraphrase-multilingual-mpnet-base-v2' # Specialized for semantic similarity\n",
    "# encoder_name = 'sentence-transformers/all-roberta-large-v1' # Very large (if you have memory)\n",
    "\n",
    "encoder = SentenceTransformer(encoder_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subreddits: 100\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "all_subreddits = pd.concat([train_df['subreddit'], test_df['subreddit']])\n",
    "le.fit(all_subreddits)\n",
    "\n",
    "train_subreddit_encoded = le.transform(train_df['subreddit'])\n",
    "test_subreddit_encoded = le.transform(test_df['subreddit'])\n",
    "\n",
    "print(f\"Number of unique subreddits: {len(le.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding unique rules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1889942fd3e4a76ad567ca8c97bb7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding train set\n",
      "Loading cached train body...\n",
      "Loading cached train pos1...\n",
      "Loading cached train pos2...\n",
      "Loading cached train neg1...\n",
      "Loading cached train neg2...\n",
      "\n",
      "Embedding test set\n",
      "Loading cached test body...\n",
      "Loading cached test pos1...\n",
      "Loading cached test pos2...\n",
      "Loading cached test neg1...\n",
      "Loading cached test neg2...\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding unique rules...\")\n",
    "all_unique_rules = pd.concat([train_df[rule_col], test_df[rule_col]]).unique()\n",
    "\n",
    "unique_rule_emb = embed_batch(\n",
    "    [f\"passage: {rule}\" for rule in all_unique_rules.tolist()],\n",
    "    encoder\n",
    ")\n",
    "\n",
    "rule_to_emb = dict(zip(all_unique_rules, unique_rule_emb))\n",
    "\n",
    "rule_emb = np.array([rule_to_emb[rule] for rule in train_df[rule_col]])\n",
    "\n",
    "print(\"\\nEmbedding train set\")\n",
    "train_emb = load_or_create_embeddings(\n",
    "    train_df,\n",
    "    prefix='train',\n",
    "    encoder_name=encoder_name,\n",
    "    encoder=encoder\n",
    ")\n",
    "\n",
    "body_emb     = train_emb['body_emb']\n",
    "pos1_emb     = train_emb['pos1_emb']\n",
    "pos2_emb     = train_emb['pos2_emb']\n",
    "neg1_emb     = train_emb['neg1_emb']\n",
    "neg2_emb     = train_emb['neg2_emb']\n",
    "\n",
    "rule_emb_test = np.array([rule_to_emb[rule] for rule in test_df[rule_col]])\n",
    "\n",
    "print(\"\\nEmbedding test set\")\n",
    "test_emb = load_or_create_embeddings(\n",
    "    test_df,\n",
    "    prefix='test',\n",
    "    encoder_name=encoder_name,\n",
    "    encoder=encoder\n",
    ")\n",
    "\n",
    "body_emb_test = test_emb['body_emb']\n",
    "pos1_emb_test = test_emb['pos1_emb']\n",
    "pos2_emb_test = test_emb['pos2_emb']\n",
    "neg1_emb_test = test_emb['neg1_emb']\n",
    "neg2_emb_test = test_emb['neg2_emb']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining train features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cf9804b3e44078b6ad7387232c248a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature shape: (2029, 17)\n",
      "Combining test features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db7ca72754942cfb78b2e4af8f7fc16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test feature shape: (54059, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining train features...\")\n",
    "train_features = []\n",
    "for i in tqdm(range(len(train_df))):\n",
    "    feat = combine_features(\n",
    "        body_emb[i],\n",
    "        rule_emb[i],\n",
    "        pos1_emb[i],\n",
    "        pos2_emb[i],\n",
    "        neg1_emb[i],\n",
    "        neg2_emb[i],\n",
    "        train_df.iloc[i][body_col],\n",
    "        train_subreddit_encoded[i]\n",
    "    )\n",
    "    train_features.append(feat)\n",
    "\n",
    "train_embeddings = np.array(train_features)\n",
    "print(f\"Train feature shape: {train_embeddings.shape}\")\n",
    "\n",
    "print(\"Combining test features...\")\n",
    "test_features = []\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    feat = combine_features(\n",
    "        body_emb_test[i],\n",
    "        rule_emb_test[i],\n",
    "        pos1_emb_test[i],\n",
    "        pos2_emb_test[i],\n",
    "        neg1_emb_test[i],\n",
    "        neg2_emb_test[i],\n",
    "        test_df.iloc[i][body_col],\n",
    "        test_subreddit_encoded[i]\n",
    "    )\n",
    "    test_features.append(feat)\n",
    "\n",
    "test_embeddings = np.array(test_features)\n",
    "print(f\"Test feature shape: {test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=384):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 38,401\n"
     ]
    }
   ],
   "source": [
    "input_dim = train_embeddings.shape[1]\n",
    "model = MLPClassifier(input_dim=input_dim).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,623\n",
      "Val: 406\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_embeddings,\n",
    "    train_df['rule_violation'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df['rule_violation']\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train):,}\")\n",
    "print(f\"Val: {len(X_val):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Trianing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6 - Loss: 0.6743 - Val AUC: 0.6678\n",
      "Epoch 2/6 - Loss: 0.6677 - Val AUC: 0.6649\n",
      "Epoch 3/6 - Loss: 0.6686 - Val AUC: 0.6590\n",
      "Epoch 4/6 - Loss: 0.6604 - Val AUC: 0.6602\n",
      "Epoch 5/6 - Loss: 0.6601 - Val AUC: 0.6534\n",
      "Epoch 6/6 - Loss: 0.6608 - Val AUC: 0.6508\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 6#10\n",
    "batch_size = 64\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = torch.FloatTensor(X_train[i:i+batch_size]).to(device)\n",
    "        batch_y = torch.FloatTensor(y_train[i:i+batch_size]).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = []\n",
    "        for i in range(0, len(X_val), batch_size):\n",
    "            batch_X = torch.FloatTensor(X_val[i:i+batch_size]).to(device)\n",
    "            outputs = model(batch_X)\n",
    "            val_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "    \n",
    "    val_auc = roc_auc_score(y_val, val_preds)\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(X_train)*batch_size:.4f} - Val AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test set...\n",
      "Test AUC: 0.60164\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting on test set...\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(test_embeddings, dtype=torch.float32, device=device)\n",
    "    logits = model(X_test_tensor)\n",
    "    test_probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "\n",
    "test_results = test_df[[\"row_id\"]].copy()\n",
    "test_results[\"prediction\"] = test_probs\n",
    "\n",
    "test_results = test_results.merge(\n",
    "    solution_df[[\"row_id\", \"rule_violation\"]],\n",
    "    on=\"row_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "y_test = test_results[\"rule_violation\"].values\n",
    "\n",
    "test_auc = roc_auc_score(y_test, test_probs)\n",
    "print(f\"Test AUC: {test_auc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP submission saved to: submissions/submission_mlp.csv\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "\n",
    "submission_path = \"submissions/submission_mlp.csv\"\n",
    "test_results[[\"row_id\", \"prediction\"]].to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"MLP submission saved to: {submission_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop per Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_list = [\n",
    "   './e5-large-v2-triplet',\n",
    "    'all-MiniLM-L12-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'paraphrase-multilingual-mpnet-base-v2',\n",
    "    'sentence-transformers/all-roberta-large-v1',\n",
    "]\n",
    "\n",
    "results_mlp = {}\n",
    "os.makedirs(\"submissions\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Running encoder: ./e5-large-v2-triplet\n",
      "==========================================================================================\n",
      "Embedding unique rules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acc101d50874583b6ad3be88e2385b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding TRAIN...\n",
      "Loading cached train body...\n",
      "Loading cached train pos1...\n",
      "Loading cached train pos2...\n",
      "Loading cached train neg1...\n",
      "Loading cached train neg2...\n",
      "\n",
      "Embedding TEST...\n",
      "Loading cached test body...\n",
      "Loading cached test pos1...\n",
      "Loading cached test pos2...\n",
      "Loading cached test neg1...\n",
      "Loading cached test neg2...\n",
      "\n",
      "Combining TRAIN features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b191d6a99146799f4531ec7135f658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining TEST features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e33e939a144e108902e6258b3ef567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP...\n",
      "Epoch 1/6 | Loss: 18.0473 | Val AUC: 0.7601\n",
      "Epoch 2/6 | Loss: 16.8343 | Val AUC: 0.8532\n",
      "Epoch 3/6 | Loss: 15.1072 | Val AUC: 0.9008\n",
      "Epoch 4/6 | Loss: 13.0482 | Val AUC: 0.9222\n",
      "Epoch 5/6 | Loss: 11.8672 | Val AUC: 0.9358\n",
      "Epoch 6/6 | Loss: 11.1940 | Val AUC: 0.9372\n",
      "Validation AUC: 0.93716\n",
      "\n",
      "Predicting on TEST...\n",
      "TEST AUC: 0.77084\n",
      "Saved → submissions/submission_mlp_._e5-large-v2-triplet.csv\n",
      "\n",
      "==========================================================================================\n",
      "Running encoder: all-MiniLM-L12-v2\n",
      "==========================================================================================\n",
      "Embedding unique rules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7b678c04f74166b368561507377878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding TRAIN...\n",
      "Loading cached train body...\n",
      "Loading cached train pos1...\n",
      "Loading cached train pos2...\n",
      "Loading cached train neg1...\n",
      "Loading cached train neg2...\n",
      "\n",
      "Embedding TEST...\n",
      "Loading cached test body...\n",
      "Loading cached test pos1...\n",
      "Loading cached test pos2...\n",
      "Loading cached test neg1...\n",
      "Loading cached test neg2...\n",
      "\n",
      "Combining TRAIN features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b2bc03a9474f79b02654c7dcdc998f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining TEST features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c992d269f7eb4ddc97faab1719d555e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP...\n",
      "Epoch 1/6 | Loss: 18.1818 | Val AUC: 0.6481\n",
      "Epoch 2/6 | Loss: 17.8405 | Val AUC: 0.6575\n",
      "Epoch 3/6 | Loss: 17.6166 | Val AUC: 0.6672\n",
      "Epoch 4/6 | Loss: 17.4945 | Val AUC: 0.6718\n",
      "Epoch 5/6 | Loss: 17.3749 | Val AUC: 0.6750\n",
      "Epoch 6/6 | Loss: 17.3584 | Val AUC: 0.6770\n",
      "Validation AUC: 0.67704\n",
      "\n",
      "Predicting on TEST...\n",
      "TEST AUC: 0.60485\n",
      "Saved → submissions/submission_mlp_all-MiniLM-L12-v2.csv\n",
      "\n",
      "==========================================================================================\n",
      "Running encoder: all-mpnet-base-v2\n",
      "==========================================================================================\n",
      "Embedding unique rules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60beee4044cc4665b88e1dfa8d96be7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding TRAIN...\n",
      "Loading cached train body...\n",
      "Loading cached train pos1...\n",
      "Loading cached train pos2...\n",
      "Loading cached train neg1...\n",
      "Loading cached train neg2...\n",
      "\n",
      "Embedding TEST...\n",
      "Loading cached test body...\n",
      "Loading cached test pos1...\n",
      "Loading cached test pos2...\n",
      "Loading cached test neg1...\n",
      "Loading cached test neg2...\n",
      "\n",
      "Combining TRAIN features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a51304dd724f77ae70a9e29d1396cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining TEST features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bdea06d9e0429e8beec9a813a9f66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP...\n",
      "Epoch 1/6 | Loss: 18.1221 | Val AUC: 0.6263\n",
      "Epoch 2/6 | Loss: 17.7965 | Val AUC: 0.6338\n",
      "Epoch 3/6 | Loss: 17.7827 | Val AUC: 0.6448\n",
      "Epoch 4/6 | Loss: 17.5377 | Val AUC: 0.6519\n",
      "Epoch 5/6 | Loss: 17.4445 | Val AUC: 0.6585\n",
      "Epoch 6/6 | Loss: 17.3091 | Val AUC: 0.6597\n",
      "Validation AUC: 0.65973\n",
      "\n",
      "Predicting on TEST...\n",
      "TEST AUC: 0.59380\n",
      "Saved → submissions/submission_mlp_all-mpnet-base-v2.csv\n",
      "\n",
      "==========================================================================================\n",
      "Running encoder: paraphrase-multilingual-mpnet-base-v2\n",
      "==========================================================================================\n",
      "Embedding unique rules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47879186b304f1496b7a01be70a9f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding TRAIN...\n",
      "Loading cached train body...\n",
      "Loading cached train pos1...\n",
      "Loading cached train pos2...\n",
      "Loading cached train neg1...\n",
      "Loading cached train neg2...\n",
      "\n",
      "Embedding TEST...\n",
      "Loading cached test body...\n",
      "Loading cached test pos1...\n",
      "Loading cached test pos2...\n",
      "Loading cached test neg1...\n",
      "Loading cached test neg2...\n",
      "\n",
      "Combining TRAIN features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b751301b2bf84487b770e76787124504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining TEST features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6640de6a4a8b4c938dfc8d0f41928bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP...\n",
      "Epoch 1/6 | Loss: 18.4477 | Val AUC: 0.6492\n",
      "Epoch 2/6 | Loss: 17.8084 | Val AUC: 0.6478\n",
      "Epoch 3/6 | Loss: 17.7809 | Val AUC: 0.6582\n",
      "Epoch 4/6 | Loss: 17.7283 | Val AUC: 0.6644\n",
      "Epoch 5/6 | Loss: 17.4395 | Val AUC: 0.6623\n",
      "Epoch 6/6 | Loss: 17.4207 | Val AUC: 0.6581\n",
      "Validation AUC: 0.65813\n",
      "\n",
      "Predicting on TEST...\n",
      "TEST AUC: 0.60039\n",
      "Saved → submissions/submission_mlp_paraphrase-multilingual-mpnet-base-v2.csv\n",
      "\n",
      "==========================================================================================\n",
      "Running encoder: sentence-transformers/all-roberta-large-v1\n",
      "==========================================================================================\n",
      "Embedding unique rules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707926596fed493eaa9e260867497c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding TRAIN...\n",
      "Loading cached train body...\n",
      "Loading cached train pos1...\n",
      "Loading cached train pos2...\n",
      "Loading cached train neg1...\n",
      "Loading cached train neg2...\n",
      "\n",
      "Embedding TEST...\n",
      "Loading cached test body...\n",
      "Loading cached test pos1...\n",
      "Loading cached test pos2...\n",
      "Loading cached test neg1...\n",
      "Loading cached test neg2...\n",
      "\n",
      "Combining TRAIN features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6500c79cbc42e790731dc470d1f84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining TEST features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a82407532a04856a37d1474f5203910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP...\n",
      "Epoch 1/6 | Loss: 18.3385 | Val AUC: 0.6381\n",
      "Epoch 2/6 | Loss: 17.8691 | Val AUC: 0.6449\n",
      "Epoch 3/6 | Loss: 17.7179 | Val AUC: 0.6519\n",
      "Epoch 4/6 | Loss: 17.6116 | Val AUC: 0.6615\n",
      "Epoch 5/6 | Loss: 17.2910 | Val AUC: 0.6638\n",
      "Epoch 6/6 | Loss: 17.3950 | Val AUC: 0.6682\n",
      "Validation AUC: 0.66816\n",
      "\n",
      "Predicting on TEST...\n",
      "TEST AUC: 0.60845\n",
      "Saved → submissions/submission_mlp_sentence-transformers_all-roberta-large-v1.csv\n"
     ]
    }
   ],
   "source": [
    "for encoder_name in encoder_list:\n",
    "\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"Running encoder: {encoder_name}\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    encoder = SentenceTransformer(encoder_name)\n",
    "\n",
    "    print(\"Embedding unique rules...\")\n",
    "    unique_rules = pd.concat([train_df[rule_col], test_df[rule_col]]).unique()\n",
    "    unique_rule_emb = embed_batch([f\"passage: {r}\" for r in unique_rules], encoder)\n",
    "    rule_to_emb = dict(zip(unique_rules, unique_rule_emb))\n",
    "\n",
    "    rule_emb_train = np.array([rule_to_emb[r] for r in train_df[rule_col]])\n",
    "    rule_emb_test  = np.array([rule_to_emb[r] for r in test_df[rule_col]])\n",
    "\n",
    "    print(\"\\nEmbedding TRAIN...\")\n",
    "    train_emb = load_or_create_embeddings(train_df, \"train\", encoder_name, encoder)\n",
    "\n",
    "    print(\"\\nEmbedding TEST...\")\n",
    "    test_emb  = load_or_create_embeddings(test_df,  \"test\",  encoder_name, encoder)\n",
    "\n",
    "    body_emb = train_emb[\"body_emb\"]\n",
    "    pos1_emb = train_emb[\"pos1_emb\"]\n",
    "    pos2_emb = train_emb[\"pos2_emb\"]\n",
    "    neg1_emb = train_emb[\"neg1_emb\"]\n",
    "    neg2_emb = train_emb[\"neg2_emb\"]\n",
    "\n",
    "    body_emb_test = test_emb[\"body_emb\"]\n",
    "    pos1_emb_test = test_emb[\"pos1_emb\"]\n",
    "    pos2_emb_test = test_emb[\"pos2_emb\"]\n",
    "    neg1_emb_test = test_emb[\"neg1_emb\"]\n",
    "    neg2_emb_test = test_emb[\"neg2_emb\"]\n",
    "\n",
    "    print(\"\\nCombining TRAIN features...\")\n",
    "    train_features = []\n",
    "    for i in tqdm(range(len(train_df))):\n",
    "        train_features.append(\n",
    "            combine_features(\n",
    "                body_emb[i],\n",
    "                rule_emb_train[i],\n",
    "                pos1_emb[i],\n",
    "                pos2_emb[i],\n",
    "                neg1_emb[i],\n",
    "                neg2_emb[i],\n",
    "                train_df.iloc[i][body_col],\n",
    "                train_subreddit_encoded[i]\n",
    "            )\n",
    "        )\n",
    "    train_embeddings = np.array(train_features)\n",
    "\n",
    "    print(\"Combining TEST features...\")\n",
    "    test_features = []\n",
    "    for i in tqdm(range(len(test_df))):\n",
    "        test_features.append(\n",
    "            combine_features(\n",
    "                body_emb_test[i],\n",
    "                rule_emb_test[i],\n",
    "                pos1_emb_test[i],\n",
    "                pos2_emb_test[i],\n",
    "                neg1_emb_test[i],\n",
    "                neg2_emb_test[i],\n",
    "                test_df.iloc[i][body_col],\n",
    "                test_subreddit_encoded[i]\n",
    "            )\n",
    "        )\n",
    "    test_embeddings = np.array(test_features)\n",
    "\n",
    "    X_train, X_val, y_train_split, y_val_split = train_test_split(\n",
    "        train_embeddings,\n",
    "        train_df[label_col].values,\n",
    "        test_size=0.2,\n",
    "        random_state=SEED,\n",
    "        stratify=train_df[label_col]\n",
    "    )\n",
    "\n",
    "    print(\"\\nTraining MLP...\")\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = MLPClassifier(input_dim=input_dim).to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    batch_size = 64\n",
    "    epochs = 6\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            bx = torch.FloatTensor(X_train[i:i+batch_size]).to(device)\n",
    "            by = torch.FloatTensor(y_train_split[i:i+batch_size]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(bx)\n",
    "            loss = criterion(out, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X_val), batch_size):\n",
    "                bx = torch.FloatTensor(X_val[i:i+batch_size]).to(device)\n",
    "                out = model(bx)\n",
    "                val_preds.extend(torch.sigmoid(out).cpu().numpy())\n",
    "\n",
    "        val_auc = roc_auc_score(y_val_split, val_preds)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "    print(f\"Validation AUC: {val_auc:.5f}\")\n",
    "\n",
    "    print(\"\\nPredicting on TEST...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.FloatTensor(test_embeddings).to(device))\n",
    "        test_probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "\n",
    "    test_results = pd.DataFrame({\n",
    "        \"row_id\": test_df[\"row_id\"],\n",
    "        \"prediction\": test_probs\n",
    "    })\n",
    "\n",
    "    test_results = test_results.merge(\n",
    "        solution_df[[\"row_id\", \"rule_violation\"]],\n",
    "        on=\"row_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    test_auc = roc_auc_score(test_results[\"rule_violation\"], test_results[\"prediction\"])\n",
    "    print(f\"TEST AUC: {test_auc:.5f}\")\n",
    "\n",
    "    safe_name = encoder_name.replace(\"/\", \"_\")\n",
    "    out_path = f\"submissions/submission_mlp_{safe_name}.csv\"\n",
    "    os.makedirs(\"submissions\", exist_ok=True)\n",
    "\n",
    "    test_results[[\"row_id\", \"prediction\"]].to_csv(out_path, index=False)\n",
    "    print(f\"Saved → {out_path}\")\n",
    "\n",
    "    results_mlp[encoder_name] = {\n",
    "        \"val_auc\": val_auc,\n",
    "        \"test_auc\": test_auc\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
