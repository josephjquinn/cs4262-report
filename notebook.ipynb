{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:34.245456Z",
     "start_time": "2025-12-01T01:06:29.594269Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import os"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jquinn/repos/cs4262-report/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "88b641d6",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "90e78c3653505758",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:34.254561Z",
     "start_time": "2025-12-01T01:06:34.252850Z"
    }
   },
   "source": [
    "rule_col = \"rule\"\n",
    "body_col = \"body\"  \n",
    "label_col = \"rule_violation\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "547d82fe761b8357",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:34.577067Z",
     "start_time": "2025-12-01T01:06:34.258530Z"
    }
   },
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")  \n",
    "solution = pd.read_csv(\"data/solution.csv\")  "
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "cc549e0c8adc66da",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:34.604223Z",
     "start_time": "2025-12-01T01:06:34.586985Z"
    }
   },
   "source": [
    "train.info()\n",
    "train.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2029 entries, 0 to 2028\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   row_id              2029 non-null   int64 \n",
      " 1   body                2029 non-null   object\n",
      " 2   rule                2029 non-null   object\n",
      " 3   subreddit           2029 non-null   object\n",
      " 4   positive_example_1  2029 non-null   object\n",
      " 5   positive_example_2  2029 non-null   object\n",
      " 6   negative_example_1  2029 non-null   object\n",
      " 7   negative_example_2  2029 non-null   object\n",
      " 8   rule_violation      2029 non-null   int64 \n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 142.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   row_id                                               body  \\\n",
       "0       0  Banks don't want you to know this! Click here ...   \n",
       "1       1  SD Stream [ ENG Link 1] (http://www.sportsstre...   \n",
       "2       2  Lol. Try appealing the ban and say you won't d...   \n",
       "3       3  she will come your home open her legs with  an...   \n",
       "4       4  code free tyrande --->>> [Imgur](http://i.imgu...   \n",
       "\n",
       "                                                rule      subreddit  \\\n",
       "0  No Advertising: Spam, referral links, unsolici...     Futurology   \n",
       "1  No Advertising: Spam, referral links, unsolici...  soccerstreams   \n",
       "2  No legal advice: Do not offer or request legal...   pcmasterrace   \n",
       "3  No Advertising: Spam, referral links, unsolici...            sex   \n",
       "4  No Advertising: Spam, referral links, unsolici...    hearthstone   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  If you could tell your younger self something ...   \n",
       "1  [I wanna kiss you all over! Stunning!](http://...   \n",
       "2  Don't break up with him or call the cops.  If ...   \n",
       "3  Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...   \n",
       "4   wow!! amazing reminds me of the old days.Well...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  hunt for lady for jack off in neighbourhood ht...   \n",
       "1  LOLGA.COM is One of the First Professional Onl...   \n",
       "2  It'll be dismissed: https://en.wikipedia.org/w...   \n",
       "3  tight pussy watch for your cock get her at thi...   \n",
       "4  seek for lady for sex in around http://p77.pl/...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  Watch Golden Globe Awards 2017 Live Online in ...   \n",
       "1  #Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...   \n",
       "2  Where is there a site that still works where y...   \n",
       "3  NSFW(obviously) http://spankbang.com/iy3u/vide...   \n",
       "4  must be watch movie https://sites.google.com/s...   \n",
       "\n",
       "                                  negative_example_2  rule_violation  \n",
       "0  DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...               0  \n",
       "1  [15 Amazing Hidden Features Of Google Search Y...               0  \n",
       "2  Because this statement of his is true. It isn'...               1  \n",
       "3  Good News ::Download WhatsApp 2.16.230 APK for...               1  \n",
       "4  We're streaming Pokemon Veitnamese Crystal RIG...               1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>body</th>\n",
       "      <th>rule</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>negative_example_2</th>\n",
       "      <th>rule_violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Banks don't want you to know this! Click here ...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>Futurology</td>\n",
       "      <td>If you could tell your younger self something ...</td>\n",
       "      <td>hunt for lady for jack off in neighbourhood ht...</td>\n",
       "      <td>Watch Golden Globe Awards 2017 Live Online in ...</td>\n",
       "      <td>DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SD Stream [ ENG Link 1] (http://www.sportsstre...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>soccerstreams</td>\n",
       "      <td>[I wanna kiss you all over! Stunning!](http://...</td>\n",
       "      <td>LOLGA.COM is One of the First Professional Onl...</td>\n",
       "      <td>#Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...</td>\n",
       "      <td>[15 Amazing Hidden Features Of Google Search Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lol. Try appealing the ban and say you won't d...</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Don't break up with him or call the cops.  If ...</td>\n",
       "      <td>It'll be dismissed: https://en.wikipedia.org/w...</td>\n",
       "      <td>Where is there a site that still works where y...</td>\n",
       "      <td>Because this statement of his is true. It isn'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she will come your home open her legs with  an...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>sex</td>\n",
       "      <td>Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...</td>\n",
       "      <td>tight pussy watch for your cock get her at thi...</td>\n",
       "      <td>NSFW(obviously) http://spankbang.com/iy3u/vide...</td>\n",
       "      <td>Good News ::Download WhatsApp 2.16.230 APK for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>hearthstone</td>\n",
       "      <td>wow!! amazing reminds me of the old days.Well...</td>\n",
       "      <td>seek for lady for sex in around http://p77.pl/...</td>\n",
       "      <td>must be watch movie https://sites.google.com/s...</td>\n",
       "      <td>We're streaming Pokemon Veitnamese Crystal RIG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:34.645840Z",
     "start_time": "2025-12-01T01:06:34.626849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test.info()\n",
    "test.head()"
   ],
   "id": "f88acfceb9dd268",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54059 entries, 0 to 54058\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   row_id              54059 non-null  int64 \n",
      " 1   body                54059 non-null  object\n",
      " 2   rule                54059 non-null  object\n",
      " 3   subreddit           54059 non-null  object\n",
      " 4   positive_example_1  54059 non-null  object\n",
      " 5   positive_example_2  54059 non-null  object\n",
      " 6   negative_example_1  54059 non-null  object\n",
      " 7   negative_example_2  54059 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 3.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   row_id                                               body  \\\n",
       "0    2029  Hi Ken l! /u/Whimsical_wispers told me he want...   \n",
       "1    2030  Doubt anyone will read this but anti depressan...   \n",
       "2    2031  **[One of her best show](http://shortora.com/i...   \n",
       "3    2032  If he'd cheat on you he'll cheat on her. She's...   \n",
       "4    2033  I think I remember reading somewhere that tech...   \n",
       "\n",
       "                                                rule      subreddit  \\\n",
       "0  No promotion of illegal activity: Do not encou...           IAmA   \n",
       "1  No medical advice: Do not offer or request spe...        science   \n",
       "2  No Advertising: Spam, referral links, unsolici...       NSFW_GIF   \n",
       "3  No spoilers: Do not reveal important details t...  relationships   \n",
       "4  No legal advice: Do not offer or request legal...      AskReddit   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  There are several web sites on internet that a...   \n",
       "1  Maybe he just has alzheimer's. It would explai...   \n",
       "2  Son Dakika Ek KazanÃ§ VideolarÄ±mÄ±zÄ± Ä°zleyin :\\n...   \n",
       "3  And now Jessica is gone.  They really need to ...   \n",
       "4  I mean, even if they didn't have anything sign...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  Someone needs to rape that judge and see how w...   \n",
       "1  Hmm well I WAS going to ask women of Reddit ho...   \n",
       "2  Try My New Game..! :)\\nhttps://play.google.com...   \n",
       "3  Probably a meme at this point, particularly th...   \n",
       "4  Fire her. You can't be sued for it, only the c...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  call whores watch for you herein http://onj.me...   \n",
       "1  You could always just carry your kid and get t...   \n",
       "2  **HD** Stream[English Tiwtch](http://www.zifoo...   \n",
       "3  hahah. Please, Barry changed his name to hide ...   \n",
       "4  Personally. I'd be wrong to tell you suicide i...   \n",
       "\n",
       "                                  negative_example_2  \n",
       "0  52 http://MySexFind.com - find girl for sex no...  \n",
       "1  Maybe you should get started on the solution a...  \n",
       "2  /r/krat0m <-subscribe for free kilo of strong ...  \n",
       "3  I found the full video.\\nThere were a few peop...  \n",
       "4  I call it salary, perhaps a better term is sch...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>body</th>\n",
       "      <th>rule</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>negative_example_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2029</td>\n",
       "      <td>Hi Ken l! /u/Whimsical_wispers told me he want...</td>\n",
       "      <td>No promotion of illegal activity: Do not encou...</td>\n",
       "      <td>IAmA</td>\n",
       "      <td>There are several web sites on internet that a...</td>\n",
       "      <td>Someone needs to rape that judge and see how w...</td>\n",
       "      <td>call whores watch for you herein http://onj.me...</td>\n",
       "      <td>52 http://MySexFind.com - find girl for sex no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2030</td>\n",
       "      <td>Doubt anyone will read this but anti depressan...</td>\n",
       "      <td>No medical advice: Do not offer or request spe...</td>\n",
       "      <td>science</td>\n",
       "      <td>Maybe he just has alzheimer's. It would explai...</td>\n",
       "      <td>Hmm well I WAS going to ask women of Reddit ho...</td>\n",
       "      <td>You could always just carry your kid and get t...</td>\n",
       "      <td>Maybe you should get started on the solution a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2031</td>\n",
       "      <td>**[One of her best show](http://shortora.com/i...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>NSFW_GIF</td>\n",
       "      <td>Son Dakika Ek KazanÃ§ VideolarÄ±mÄ±zÄ± Ä°zleyin :\\n...</td>\n",
       "      <td>Try My New Game..! :)\\nhttps://play.google.com...</td>\n",
       "      <td>**HD** Stream[English Tiwtch](http://www.zifoo...</td>\n",
       "      <td>/r/krat0m &lt;-subscribe for free kilo of strong ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2032</td>\n",
       "      <td>If he'd cheat on you he'll cheat on her. She's...</td>\n",
       "      <td>No spoilers: Do not reveal important details t...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>And now Jessica is gone.  They really need to ...</td>\n",
       "      <td>Probably a meme at this point, particularly th...</td>\n",
       "      <td>hahah. Please, Barry changed his name to hide ...</td>\n",
       "      <td>I found the full video.\\nThere were a few peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2033</td>\n",
       "      <td>I think I remember reading somewhere that tech...</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>I mean, even if they didn't have anything sign...</td>\n",
       "      <td>Fire her. You can't be sued for it, only the c...</td>\n",
       "      <td>Personally. I'd be wrong to tell you suicide i...</td>\n",
       "      <td>I call it salary, perhaps a better term is sch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9ed73246eb091d56",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-01T01:01:50.841264Z",
     "start_time": "2025-12-01T01:01:50.798991Z"
    }
   },
   "source": [
    "print(\"\\nTraining Set\")\n",
    "print(f\"Total samples: {len(train)}\")\n",
    "print(f\"Class distribution:\\n{train[label_col].value_counts()}\")\n",
    "print(f\"Class ratio: {train[label_col].value_counts(normalize=True)}\")\n",
    "print(f\"\\nUnique rules in train: {train[rule_col].nunique()}\")\n",
    "print(f\"Rules: {train[rule_col].unique()}\")\n",
    "print(f\"\\nUnique subreddits in train: {train['subreddit'].nunique()}\")\n",
    "\n",
    "print(\"\\nTest Set\")\n",
    "print(f\"Total samples: {len(test)}\")\n",
    "print(f\"\\nUnique rules in test: {test[rule_col].nunique()}\")\n",
    "print(f\"Rules: {test[rule_col].unique()}\")\n",
    "print(f\"\\nUnique subreddits in test: {test['subreddit'].nunique()}\")\n",
    "\n",
    "print(\"\\nAnalysis\")\n",
    "train_rules = set(train[rule_col].unique())\n",
    "test_rules = set(test[rule_col].unique())\n",
    "print(f\"Rules in test but NOT in train: {test_rules - train_rules}\")\n",
    "print(f\"Number of test samples with unseen rules: {test[~test[rule_col].isin(train_rules)].shape[0]}\")\n",
    "print(f\"Percentage: {100 * test[~test[rule_col].isin(train_rules)].shape[0] / len(test):.2f}%\")\n",
    "\n",
    "train_subreddits = set(train['subreddit'].unique())\n",
    "test_subreddits = set(test['subreddit'].unique())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set\n",
      "Total samples: 2029\n",
      "Class distribution:\n",
      "rule_violation\n",
      "1    1031\n",
      "0     998\n",
      "Name: count, dtype: int64\n",
      "Class ratio: rule_violation\n",
      "1    0.508132\n",
      "0    0.491868\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Unique rules in train: 2\n",
      "Rules: ['No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.'\n",
      " 'No legal advice: Do not offer or request legal advice.']\n",
      "\n",
      "Unique subreddits in train: 100\n",
      "\n",
      "Test Set\n",
      "Total samples: 54059\n",
      "\n",
      "Unique rules in test: 6\n",
      "Rules: ['No promotion of illegal activity: Do not encourage or promote illegal activities, such as drug-related activity, violence, exploitation, theft, or other criminal behavior.'\n",
      " 'No medical advice: Do not offer or request specific medical advice, diagnoses, or treatment recommendations.'\n",
      " 'No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.'\n",
      " \"No spoilers: Do not reveal important details that would limit people's ability to enjoy a show or movie.\"\n",
      " 'No legal advice: Do not offer or request legal advice.'\n",
      " 'No financial advice: We do not permit comments that make personal recommendations for investments, taxes, or careers.']\n",
      "\n",
      "Unique subreddits in test: 100\n",
      "\n",
      "Analysis\n",
      "Rules in test but NOT in train: {'No financial advice: We do not permit comments that make personal recommendations for investments, taxes, or careers.', \"No spoilers: Do not reveal important details that would limit people's ability to enjoy a show or movie.\", 'No promotion of illegal activity: Do not encourage or promote illegal activities, such as drug-related activity, violence, exploitation, theft, or other criminal behavior.', 'No medical advice: Do not offer or request specific medical advice, diagnoses, or treatment recommendations.'}\n",
      "Number of test samples with unseen rules: 36088\n",
      "Percentage: 66.76%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "During our data analysis we noticed a few pain points in solving this problem. First, we have a small training dataset of 2,029 samples, and we are evaluating this on a test set of 54,059 samples. Secondly, we have an additional 4 rules in our test dataset that are unseen in our training dataset, these unseen rules make up 66.76% of our test samples",
   "id": "5a3361ddcbd486d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Helper Functions",
   "id": "f40c47e0387c7bcd"
  },
  {
   "cell_type": "code",
   "id": "91f5655cab46450b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:39.993742Z",
     "start_time": "2025-12-01T01:06:38.269792Z"
    }
   },
   "source": [
    "device = 'cuda' if os.system('nvidia-smi > /dev/null 2>&1') == 0 else 'mps' if os.system('sysctl -n machdep.cpu.brand_string | grep -q Apple > /dev/null 2>&1') == 0 else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n",
    "\n",
    "def embed_batch(texts):\n",
    "    return model.encode(texts, normalize_embeddings=True, show_progress_bar=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:41.211668Z",
     "start_time": "2025-12-01T01:06:41.207563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_or_create_embeddings(df, prefix, cache_dir=\"embeddings_cache\"):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    embeddings = {}\n",
    "\n",
    "    to_embed = {\n",
    "        'body': body_col,\n",
    "        'pos1': 'positive_example_1',\n",
    "        'pos2': 'positive_example_2',\n",
    "        'neg1': 'negative_example_1',\n",
    "        'neg2': 'negative_example_2'\n",
    "    }\n",
    "\n",
    "    for key, col in to_embed.items():\n",
    "        cache_file = f\"{cache_dir}/{prefix}_{key}_emb.pkl\"\n",
    "\n",
    "        if os.path.exists(cache_file):\n",
    "            print(f\"Loading cached {prefix} {key} embeddings...\")\n",
    "            embeddings[f\"{key}_emb\"] = pickle.load(open(cache_file, 'rb'))\n",
    "        else:\n",
    "            print(f\"Embedding {prefix} {col}...\")\n",
    "            embeddings[f\"{key}_emb\"] = embed_batch(df[col].tolist())\n",
    "            pickle.dump(embeddings[f\"{key}_emb\"], open(cache_file, 'wb'))\n",
    "\n",
    "    return embeddings\n",
    "\n"
   ],
   "id": "b95f12a6fe4a8e32",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:42.643371Z",
     "start_time": "2025-12-01T01:06:42.640042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def extract_text_features(text):\n",
    "    num_chars = len(text)\n",
    "    num_words = len(text.split())\n",
    "\n",
    "    has_url = 1 if re.search(r'http[s]?://|www\\.', text) else 0\n",
    "\n",
    "    if num_chars > 0:\n",
    "        caps_ratio = sum(1 for c in text if c.isupper()) / num_chars\n",
    "    else:\n",
    "        caps_ratio = 0\n",
    "\n",
    "    return np.array([num_chars, num_words, has_url, caps_ratio])"
   ],
   "id": "567fd704fe5596f9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:44.161731Z",
     "start_time": "2025-12-01T01:06:44.157444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combine_features(body, rule, pos1, pos2, neg1, neg2, text, subreddit_encoded):\n",
    "    sim_pos1 = (body * pos1).sum()\n",
    "    sim_pos2 = (body * pos2).sum()\n",
    "    sim_neg1 = (body * neg1).sum()\n",
    "    sim_neg2 = (body * neg2).sum()\n",
    "    sim_rule = (body * rule).sum()\n",
    "\n",
    "    pos_avg = (sim_pos1 + sim_pos2) / 2\n",
    "    neg_avg = (sim_neg1 + sim_neg2) / 2\n",
    "    pos_neg_ratio = pos_avg / (neg_avg + 1e-6)\n",
    "    pos_neg_diff = pos_avg - neg_avg\n",
    "\n",
    "    text_feats = extract_text_features(text)\n",
    "\n",
    "    return np.concatenate([\n",
    "        [sim_pos1, sim_pos2, sim_neg1, sim_neg2, sim_rule],  # 5 features\n",
    "        [pos_neg_ratio, pos_neg_diff],  # 2 features\n",
    "        text_feats,  # 4 features (num_chars, num_words, has_url, caps_ratio)\n",
    "        [subreddit_encoded]  # 1 feature\n",
    "    ])"
   ],
   "id": "668eab8304c3e523",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "571ff98694bbcb91",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-01T01:07:21.415276Z",
     "start_time": "2025-12-01T01:07:20.341659Z"
    }
   },
   "source": [
    "print(\"Embedding unique rules...\")\n",
    "all_unique_rules = pd.concat([train[rule_col], test[rule_col]]).unique()\n",
    "unique_rule_emb = embed_batch(all_unique_rules.tolist())\n",
    "rule_to_emb = dict(zip(all_unique_rules, unique_rule_emb))\n",
    "rule_emb = np.array([rule_to_emb[rule] for rule in train[rule_col]])\n",
    "\n",
    "print(\"\\nEmbedding train set\")\n",
    "train_emb = load_or_create_embeddings(train, prefix='train')\n",
    "body_emb = train_emb['body_emb']\n",
    "pos1_emb = train_emb['pos1_emb']\n",
    "pos2_emb = train_emb['pos2_emb']\n",
    "neg1_emb = train_emb['neg1_emb']\n",
    "neg2_emb = train_emb['neg2_emb']\n",
    "rule_emb_test = np.array([rule_to_emb[rule] for rule in test[rule_col]])\n",
    "\n",
    "print(\"\\nEmbedding test set\")\n",
    "test_emb = load_or_create_embeddings(test, prefix='test')\n",
    "body_emb_test = test_emb['body_emb']\n",
    "pos1_emb_test = test_emb['pos1_emb']\n",
    "pos2_emb_test = test_emb['pos2_emb']\n",
    "neg1_emb_test = test_emb['neg1_emb']\n",
    "neg2_emb_test = test_emb['neg2_emb']"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding unique rules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding train set\n",
      "Loading cached train body embeddings...\n",
      "Loading cached train pos1 embeddings...\n",
      "Loading cached train pos2 embeddings...\n",
      "Loading cached train neg1 embeddings...\n",
      "Loading cached train neg2 embeddings...\n",
      "\n",
      "Embedding test set\n",
      "Loading cached test body embeddings...\n",
      "Loading cached test pos1 embeddings...\n",
      "Loading cached test pos2 embeddings...\n",
      "Loading cached test neg1 embeddings...\n",
      "Loading cached test neg2 embeddings...\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:07:36.531238Z",
     "start_time": "2025-12-01T01:07:36.490491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subreddit_encoder = LabelEncoder()\n",
    "subreddit_encoder.fit(pd.concat([train['subreddit'], test['subreddit']]))"
   ],
   "id": "d0d4a7933f3a9bbc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LabelEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "ff9792c583b459bc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-01T01:07:37.946889Z",
     "start_time": "2025-12-01T01:07:37.881873Z"
    }
   },
   "source": [
    "train_subreddit_encoded = subreddit_encoder.transform(train['subreddit'])\n",
    "y = train[label_col].values\n",
    "\n",
    "X = np.array([\n",
    "    combine_features(\n",
    "        body_emb[i], rule_emb[i], \n",
    "        pos1_emb[i], pos2_emb[i], \n",
    "        neg1_emb[i], neg2_emb[i],\n",
    "        train[body_col].iloc[i],\n",
    "        train_subreddit_encoded[i]\n",
    "    ) \n",
    "    for i in range(len(train))\n",
    "])\n",
    "\n",
    "\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "X.shape, y.shape"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (2029, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2029, 12), (2029,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "89373b0e9845959",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-01T01:07:41.027919Z",
     "start_time": "2025-12-01T01:07:39.519997Z"
    }
   },
   "source": [
    "test_subreddit_encoded = subreddit_encoder.transform(test['subreddit'])\n",
    "\n",
    "X_test = np.array([\n",
    "    combine_features(\n",
    "        body_emb_test[i], rule_emb_test[i],\n",
    "        pos1_emb_test[i], pos2_emb_test[i],\n",
    "        neg1_emb_test[i], neg2_emb_test[i],\n",
    "        test[body_col].iloc[i],\n",
    "        test_subreddit_encoded[i]\n",
    "    )\n",
    "    for i in range(len(test))\n",
    "])\n",
    "\n",
    "print(f\"Test features shape: {X_test.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features shape: (54059, 12)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Main Training Loop",
   "id": "e43e45c05cc3fd51"
  },
  {
   "cell_type": "code",
   "id": "cc6f743b989ed016",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-01T01:08:54.070420Z",
     "start_time": "2025-12-01T01:08:44.305423Z"
    }
   },
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, n_jobs=-1, class_weight=\"balanced\"),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=10, n_jobs=-1, class_weight=\"balanced\", random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=200, max_depth=4, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, reg_alpha=1.0, reg_lambda=1.0, random_state=42, n_jobs=-1, eval_metric='logloss'),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    test_preds = model.predict(X_test)\n",
    "    test_probs = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    test_results = test[[\"row_id\"]].copy()\n",
    "    test_results[\"prediction\"] = test_probs\n",
    "    test_results = test_results.merge(solution[[\"row_id\", \"rule_violation\"]], on=\"row_id\", how=\"left\")\n",
    "    y_test = test_results[\"rule_violation\"].values\n",
    "    \n",
    "    print(f\"Metrics on test dataset\")\n",
    "    print(classification_report(y_test, test_preds))\n",
    "    test_auc = roc_auc_score(y_test, test_probs)\n",
    "    test_acc = (test_preds == y_test).mean()\n",
    "    print(f\"AUC: {test_auc:.4f}\")\n",
    "    print(f\"Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    results[name] = {\n",
    "        'test_auc': test_auc,\n",
    "        'test_acc': test_acc,\n",
    "        'model': model,\n",
    "        'test_probs': test_probs\n",
    "    }\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Summary\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:20s} | Test AUC: {res['test_auc']:.4f} | Test Acc: {res['test_acc']:.4f}\")\n",
    "\n",
    "# best_model_name = max(results.items(), key=lambda x: x[1]['test_auc'])[0]\n",
    "# best_probs = results[best_model_name]['test_probs']\n",
    "# test[[\"row_id\"]].assign(prediction=best_probs).to_csv(\"submission.csv\", index=False)\n",
    "# print(f\"\\nSaved submission.csv using {best_model_name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68     28789\n",
      "           1       0.64      0.63      0.63     25270\n",
      "\n",
      "    accuracy                           0.66     54059\n",
      "   macro avg       0.66      0.66      0.66     54059\n",
      "weighted avg       0.66      0.66      0.66     54059\n",
      "\n",
      "AUC: 0.7194\n",
      "Accuracy: 0.6589\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66     28789\n",
      "           1       0.61      0.66      0.64     25270\n",
      "\n",
      "    accuracy                           0.65     54059\n",
      "   macro avg       0.65      0.65      0.65     54059\n",
      "weighted avg       0.65      0.65      0.65     54059\n",
      "\n",
      "AUC: 0.7079\n",
      "Accuracy: 0.6476\n",
      "\n",
      "\n",
      "Training Gradient Boosting...\n",
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.63      0.64     28789\n",
      "           1       0.59      0.60      0.60     25270\n",
      "\n",
      "    accuracy                           0.62     54059\n",
      "   macro avg       0.62      0.62      0.62     54059\n",
      "weighted avg       0.62      0.62      0.62     54059\n",
      "\n",
      "AUC: 0.6700\n",
      "Accuracy: 0.6187\n",
      "\n",
      "\n",
      "Training XGBoost...\n",
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.66     28789\n",
      "           1       0.62      0.62      0.62     25270\n",
      "\n",
      "    accuracy                           0.64     54059\n",
      "   macro avg       0.64      0.64      0.64     54059\n",
      "weighted avg       0.64      0.64      0.64     54059\n",
      "\n",
      "AUC: 0.7022\n",
      "Accuracy: 0.6430\n",
      "\n",
      "\n",
      "Training SVM (RBF)...\n",
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66     28789\n",
      "           1       0.59      0.42      0.49     25270\n",
      "\n",
      "    accuracy                           0.59     54059\n",
      "   macro avg       0.59      0.58      0.58     54059\n",
      "weighted avg       0.59      0.59      0.58     54059\n",
      "\n",
      "AUC: 0.6193\n",
      "Accuracy: 0.5943\n",
      "\n",
      "\n",
      "Summary\n",
      "Logistic Regression  | Test AUC: 0.7194 | Test Acc: 0.6589\n",
      "Random Forest        | Test AUC: 0.7079 | Test Acc: 0.6476\n",
      "Gradient Boosting    | Test AUC: 0.6700 | Test Acc: 0.6187\n",
      "XGBoost              | Test AUC: 0.7022 | Test Acc: 0.6430\n",
      "SVM (RBF)            | Test AUC: 0.6193 | Test Acc: 0.5943\n",
      "\n",
      "Saved submission.csv using Logistic Regression\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. More sophisticated similarity features",
   "id": "16c48c913240eae4"
  },
  {
   "cell_type": "code",
   "id": "d669dc28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:47:46.867724Z",
     "start_time": "2025-12-01T01:47:43.727598Z"
    }
   },
   "source": [
    "def combine_features_v2(body, rule, pos1, pos2, neg1, neg2, text, subreddit_encoded):\n",
    "    # Basic similarities \n",
    "    sim_pos1 = (body * pos1).sum()\n",
    "    sim_pos2 = (body * pos2).sum()\n",
    "    sim_neg1 = (body * neg1).sum()\n",
    "    sim_neg2 = (body * neg2).sum()\n",
    "    sim_rule = (body * rule).sum()\n",
    "    \n",
    "    # Aggregate similarities\n",
    "    pos_avg = (sim_pos1 + sim_pos2) / 2\n",
    "    neg_avg = (sim_neg1 + sim_neg2) / 2\n",
    "    pos_max = max(sim_pos1, sim_pos2)\n",
    "    pos_min = min(sim_pos1, sim_pos2)\n",
    "    neg_max = max(sim_neg1, sim_neg2)\n",
    "    neg_min = min(sim_neg1, sim_neg2)\n",
    "    \n",
    "    # Ratio and difference features\n",
    "    pos_neg_ratio = pos_avg / (neg_avg + 1e-6)\n",
    "    pos_neg_diff = pos_avg - neg_avg\n",
    "    pos_range = pos_max - pos_min  # Consistency of positive examples\n",
    "    neg_range = neg_max - neg_min  # Consistency of negative examples\n",
    "    \n",
    "    # New: Compare with rule embedding\n",
    "    rule_pos_avg_sim = (rule * ((pos1 + pos2) / 2)).sum()\n",
    "    rule_neg_avg_sim = (rule * ((neg1 + neg2) / 2)).sum()\n",
    "    \n",
    "    # New: Cross-similarities between examples\n",
    "    pos1_pos2_sim = (pos1 * pos2).sum()\n",
    "    neg1_neg2_sim = (neg1 * neg2).sum()\n",
    "    \n",
    "    # New: Distance-based features (L2 distance)\n",
    "    pos_avg_emb = (pos1 + pos2) / 2\n",
    "    neg_avg_emb = (neg1 + neg2) / 2\n",
    "    dist_to_pos = np.linalg.norm(body - pos_avg_emb)\n",
    "    dist_to_neg = np.linalg.norm(body - neg_avg_emb)\n",
    "    dist_ratio = dist_to_neg / (dist_to_pos + 1e-6)\n",
    "    \n",
    "    # Text statistical features\n",
    "    text_feats = extract_text_features(text)\n",
    "    \n",
    "    # Combine all features\n",
    "    return np.concatenate([\n",
    "        # Original similarities (5)\n",
    "        [sim_pos1, sim_pos2, sim_neg1, sim_neg2, sim_rule],\n",
    "        # Aggregate features (6)\n",
    "        [pos_avg, neg_avg, pos_max, neg_max, pos_min, neg_min],\n",
    "        # Ratio/diff features (4)\n",
    "        [pos_neg_ratio, pos_neg_diff, pos_range, neg_range],\n",
    "        # Rule-example similarities (2)\n",
    "        [rule_pos_avg_sim, rule_neg_avg_sim],\n",
    "        # Cross-similarities (2)\n",
    "        [pos1_pos2_sim, neg1_neg2_sim],\n",
    "        # Distance features (3)\n",
    "        [dist_to_pos, dist_to_neg, dist_ratio],\n",
    "        # Text features (4)\n",
    "        text_feats,\n",
    "        # Categorical (1)\n",
    "        [subreddit_encoded]\n",
    "    ])\n",
    "\n",
    "# Build improved features for train\n",
    "print(\"Building enhanced training features...\")\n",
    "X_v2 = np.array([\n",
    "    combine_features_v2(\n",
    "        body_emb[i], rule_emb[i], \n",
    "        pos1_emb[i], pos2_emb[i], \n",
    "        neg1_emb[i], neg2_emb[i],\n",
    "        train[body_col].iloc[i],\n",
    "        train_subreddit_encoded[i]\n",
    "    ) \n",
    "    for i in range(len(train))\n",
    "])\n",
    "\n",
    "print(f\"Enhanced feature shape: {X_v2.shape}\")\n",
    "print(f\"Feature breakdown: 5 basic + 6 aggregate + 4 ratio/diff + 2 rule-example + 2 cross + 3 distance + 4 text + 1 categorical = {X_v2.shape[1]} total\")\n",
    "\n",
    "# Build improved features for test\n",
    "print(\"\\nBuilding enhanced test features...\")\n",
    "X_test_v2 = np.array([\n",
    "    combine_features_v2(\n",
    "        body_emb_test[i], rule_emb_test[i],\n",
    "        pos1_emb_test[i], pos2_emb_test[i],\n",
    "        neg1_emb_test[i], neg2_emb_test[i],\n",
    "        test[body_col].iloc[i],\n",
    "        test_subreddit_encoded[i]\n",
    "    )\n",
    "    for i in range(len(test))\n",
    "])\n",
    "\n",
    "print(f\"Enhanced test feature shape: {X_test_v2.shape}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building enhanced training features...\n",
      "Enhanced feature shape: (2029, 27)\n",
      "Feature breakdown: 5 basic + 6 aggregate + 4 ratio/diff + 2 rule-example + 2 cross + 3 distance + 4 text + 1 categorical = 27 total\n",
      "\n",
      "Building enhanced test features...\n",
      "Enhanced test feature shape: (54059, 27)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:48:01.904473Z",
     "start_time": "2025-12-01T01:47:48.101253Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.70      0.68     28789\n",
      "           1       0.64      0.61      0.62     25270\n",
      "\n",
      "    accuracy                           0.66     54059\n",
      "   macro avg       0.66      0.65      0.65     54059\n",
      "weighted avg       0.66      0.66      0.66     54059\n",
      "\n",
      "AUC: 0.7159\n",
      "Accuracy: 0.6568\n",
      "\n",
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jquinn/repos/cs4262-report/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67     28789\n",
      "           1       0.63      0.69      0.66     25270\n",
      "\n",
      "    accuracy                           0.67     54059\n",
      "   macro avg       0.67      0.67      0.67     54059\n",
      "weighted avg       0.67      0.67      0.67     54059\n",
      "\n",
      "AUC: 0.7245\n",
      "Accuracy: 0.6652\n",
      "\n",
      "Training Gradient Boosting...\n",
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64     28789\n",
      "           1       0.59      0.60      0.59     25270\n",
      "\n",
      "    accuracy                           0.62     54059\n",
      "   macro avg       0.62      0.62      0.62     54059\n",
      "weighted avg       0.62      0.62      0.62     54059\n",
      "\n",
      "AUC: 0.6689\n",
      "Accuracy: 0.6197\n",
      "\n",
      "Training XGBoost...\n",
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.65      0.66     28789\n",
      "           1       0.61      0.63      0.62     25270\n",
      "\n",
      "    accuracy                           0.64     54059\n",
      "   macro avg       0.64      0.64      0.64     54059\n",
      "weighted avg       0.64      0.64      0.64     54059\n",
      "\n",
      "AUC: 0.7013\n",
      "Accuracy: 0.6411\n",
      "\n",
      "Training SVM (RBF)...\n",
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66     28789\n",
      "           1       0.59      0.42      0.49     25270\n",
      "\n",
      "    accuracy                           0.59     54059\n",
      "   macro avg       0.59      0.58      0.58     54059\n",
      "weighted avg       0.59      0.59      0.58     54059\n",
      "\n",
      "AUC: 0.6196\n",
      "Accuracy: 0.5936\n",
      "\n",
      "Summary\n",
      "Logistic Regression  | Test AUC: 0.7159 | Test Acc: 0.6568\n",
      "Random Forest        | Test AUC: 0.7245 | Test Acc: 0.6652\n",
      "Gradient Boosting    | Test AUC: 0.6689 | Test Acc: 0.6197\n",
      "XGBoost              | Test AUC: 0.7013 | Test Acc: 0.6411\n",
      "SVM (RBF)            | Test AUC: 0.6196 | Test Acc: 0.5936\n"
     ]
    }
   ],
   "execution_count": 22,
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    'SVM (RBF)': SVC(\n",
    "        kernel='rbf',\n",
    "        probability=True,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Pre-merge test labels once\n",
    "y_test = (\n",
    "    test[['row_id']]\n",
    "    .merge(solution[['row_id', 'rule_violation']], on='row_id', how='left')\n",
    "    ['rule_violation']\n",
    "    .values\n",
    ")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    # Train on the new feature set\n",
    "    model.fit(X_v2, y)\n",
    "\n",
    "    # Predict\n",
    "    test_preds = model.predict(X_test_v2)\n",
    "    test_probs = model.predict_proba(X_test_v2)[:, 1]\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"Metrics on test dataset\")\n",
    "    print(classification_report(y_test, test_preds))\n",
    "\n",
    "    test_auc = roc_auc_score(y_test, test_probs)\n",
    "    test_acc = (test_preds == y_test).mean()\n",
    "\n",
    "    print(f\"AUC: {test_auc:.4f}\")\n",
    "    print(f\"Accuracy: {test_acc:.4f}\")\n",
    "    print()\n",
    "\n",
    "    results[name] = {\n",
    "        'test_auc': test_auc,\n",
    "        'test_acc': test_acc,\n",
    "        'model': model,\n",
    "        'test_probs': test_probs\n",
    "    }\n",
    "\n",
    "print(\"Summary\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:20s} | Test AUC: {res['test_auc']:.4f} | Test Acc: {res['test_acc']:.4f}\")\n",
    "\n",
    "# Example save block (unchanged)\n",
    "# best_model_name = max(results.items(), key=lambda x: x[1]['test_auc'])[0]\n",
    "# best_probs = results[best_model_name]['test_probs']\n",
    "# test[['row_id']].assign(prediction=best_probs).to_csv(\"submission.csv\", index=False)\n",
    "# print(f\"\\nSaved submission.csv using {best_model_name}\")\n"
   ],
   "id": "45b8adaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Ensemble of Multiple Models",
   "id": "3d10fd5a3030c11"
  },
  {
   "cell_type": "code",
   "id": "1013ebd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:49:48.703463Z",
     "start_time": "2025-12-01T01:49:48.543558Z"
    }
   },
   "source": [
    "print(\"Ensamble approach\")\n",
    "\n",
    "# Create ensemble of diverse models\n",
    "ensemble_models = [\n",
    "    ('logistic', LogisticRegression(C=clf_cv.C_[0], max_iter=1000, class_weight='balanced', random_state=42)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=8, min_samples_leaf=10, \n",
    "                                   class_weight='balanced', random_state=42, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=150, max_depth=4, learning_rate=0.05, \n",
    "                                       subsample=0.8, random_state=42))\n",
    "]\n",
    "\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=ensemble_models,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "print(f\"Training ensemble of {len(ensemble_models)} models...\")\n",
    "ensemble.fit(X_v2, y)\n",
    "\n",
    "test_probs_ensemble = ensemble.predict_proba(X_test_v2)[:,1]\n",
    "test_preds_ensemble = ensemble.predict(X_test_v2)\n",
    "\n",
    "print(f\"\\nEnsemble Test Performance:\")\n",
    "print(classification_report(y_test, test_preds_ensemble))\n",
    "test_auc_ensemble = roc_auc_score(y_test, test_probs_ensemble)\n",
    "print(f\"Test AUC: {test_auc_ensemble:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensamble approach\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VotingClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 13\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Create ensemble of diverse models\u001B[39;00m\n\u001B[1;32m      4\u001B[0m ensemble_models \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      5\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogistic\u001B[39m\u001B[38;5;124m'\u001B[39m, LogisticRegression(C\u001B[38;5;241m=\u001B[39mclf_cv\u001B[38;5;241m.\u001B[39mC_[\u001B[38;5;241m0\u001B[39m], max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, class_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m'\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)),\n\u001B[1;32m      6\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m'\u001B[39m, RandomForestClassifier(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, min_samples_leaf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m                                        subsample\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m))\n\u001B[1;32m     10\u001B[0m ]\n\u001B[0;32m---> 13\u001B[0m ensemble \u001B[38;5;241m=\u001B[39m \u001B[43mVotingClassifier\u001B[49m(\n\u001B[1;32m     14\u001B[0m     estimators\u001B[38;5;241m=\u001B[39mensemble_models,\n\u001B[1;32m     15\u001B[0m     voting\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msoft\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     16\u001B[0m )\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining ensemble of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(ensemble_models)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m models...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m ensemble\u001B[38;5;241m.\u001B[39mfit(X_v2, y)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'VotingClassifier' is not defined"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7034dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADDITIONAL IMPROVEMENT IDEAS TO TRY:\n",
    "\n",
    "1. BETTER EMBEDDING MODELS:\n",
    "   - all-MiniLM-L12-v2: Faster, slightly worse quality\n",
    "   - paraphrase-mpnet-base-v2: Specifically trained for semantic similarity\n",
    "   - all-distilroberta-v1: Good balance of speed/quality\n",
    "   - sentence-t5-large: Larger model, better quality (slower)\n",
    "   \n",
    "2. RULE-SPECIFIC APPROACHES:\n",
    "   Since test has unseen rules, consider:\n",
    "   - Train separate models per rule (if enough samples)\n",
    "   - Use rule embeddings as features more heavily\n",
    "   - Compute similarity between new rules and training rules\n",
    "   \n",
    "3. DATA AUGMENTATION:\n",
    "   - Paraphrase training examples using LLMs\n",
    "   - Back-translation augmentation\n",
    "   - Mix positive/negative examples with noise\n",
    "   \n",
    "4. ITERATIVE PSEUDO-LABELING:\n",
    "   - Start with high confidence (0.95+)\n",
    "   - Retrain and gradually lower threshold\n",
    "   - Multiple iterations\n",
    "   \n",
    "5. STACKING (Meta-Learning):\n",
    "   - Train multiple base models\n",
    "   - Use their predictions as features for a meta-model\n",
    "   - Can capture complex model interactions\n",
    "   \n",
    "6. CONTRASTIVE LEARNING:\n",
    "   - Fine-tune sentence embeddings on this specific task\n",
    "   - Use positive/negative examples as training signal\n",
    "   - Create triplet loss: (body, pos_example, neg_example)\n",
    "   \n",
    "7. CALIBRATION:\n",
    "   - Use Platt scaling or isotonic regression\n",
    "   - Calibrate probabilities to better match test distribution\n",
    "   \n",
    "8. FEATURE SELECTION:\n",
    "   - Use L1 regularization to identify most important features\n",
    "   - Remove noisy features that may hurt generalization\n",
    "   \n",
    "9. ANALYZE ERRORS:\n",
    "   - Look at misclassified examples\n",
    "   - Identify patterns in what the model gets wrong\n",
    "   - Engineer features specifically for those cases\n",
    "\n",
    "To implement any of these, uncomment and modify the code in the next cells!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8l0vnxzvdm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE SELECTION USING L1 REGULARIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE SELECTION WITH L1 REGULARIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# First, check the actual feature count\n",
    "print(f\"\\nActual feature matrix shape: {X_v2.shape}\")\n",
    "print(f\"Number of features: {X_v2.shape[1]}\")\n",
    "\n",
    "# Create feature names matching the actual combine_features_v2 function\n",
    "# Based on the concatenation order in combine_features_v2\n",
    "feature_names = []\n",
    "\n",
    "# Original similarities (5)\n",
    "feature_names.extend(['sim_pos1', 'sim_pos2', 'sim_neg1', 'sim_neg2', 'sim_rule'])\n",
    "\n",
    "# Aggregate features (6)\n",
    "feature_names.extend(['pos_avg', 'neg_avg', 'pos_max', 'neg_max', 'pos_min', 'neg_min'])\n",
    "\n",
    "# Ratio/diff features (4)\n",
    "feature_names.extend(['pos_neg_ratio', 'pos_neg_diff', 'pos_range', 'neg_range'])\n",
    "\n",
    "# Rule-example similarities (2)\n",
    "feature_names.extend(['rule_pos_avg_sim', 'rule_neg_avg_sim'])\n",
    "\n",
    "# Cross-similarities (2)\n",
    "feature_names.extend(['pos1_pos2_sim', 'neg1_neg2_sim'])\n",
    "\n",
    "# Distance features (3)\n",
    "feature_names.extend(['dist_to_pos', 'dist_to_neg', 'dist_ratio'])\n",
    "\n",
    "# Text features (4)\n",
    "feature_names.extend(['num_chars', 'num_words', 'has_url', 'caps_ratio'])\n",
    "\n",
    "# Categorical (1)\n",
    "feature_names.extend(['subreddit_encoded'])\n",
    "\n",
    "# If there's still a mismatch, add placeholder names\n",
    "if len(feature_names) < X_v2.shape[1]:\n",
    "    print(f\"\\nWARNING: Expected {len(feature_names)} features but found {X_v2.shape[1]}\")\n",
    "    print(f\"Adding {X_v2.shape[1] - len(feature_names)} placeholder feature names...\")\n",
    "    for i in range(len(feature_names), X_v2.shape[1]):\n",
    "        feature_names.append(f'feature_{i}')\n",
    "elif len(feature_names) > X_v2.shape[1]:\n",
    "    print(f\"\\nWARNING: Have {len(feature_names)} feature names but only {X_v2.shape[1]} features\")\n",
    "    print(f\"Truncating to {X_v2.shape[1]} names...\")\n",
    "    feature_names = feature_names[:X_v2.shape[1]]\n",
    "\n",
    "print(f\"Feature names created: {len(feature_names)}\")\n",
    "\n",
    "# Train L1-regularized Logistic Regression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Step 1: Train L1-Regularized Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features for L1 regularization (important for fair comparison)\n",
    "scaler = StandardScaler()\n",
    "X_v2_scaled = scaler.fit_transform(X_v2)\n",
    "X_test_v2_scaled = scaler.transform(X_test_v2)\n",
    "\n",
    "# Use L1 penalty (Lasso) to shrink unimportant coefficients to zero\n",
    "clf_l1 = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=1.0,  # Regularization strength (smaller = stronger regularization)\n",
    "    solver='liblinear',  # Required for L1\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf_l1.fit(X_v2_scaled, y)\n",
    "\n",
    "# Get feature coefficients\n",
    "coefficients = clf_l1.coef_[0]\n",
    "abs_coefficients = np.abs(coefficients)\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients,\n",
    "    'abs_coefficient': abs_coefficients\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (sorted by absolute coefficient):\")\n",
    "print(feature_importance_df.to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Feature Importance Visualization\")\n",
    "print(f\"{'='*60}\")\n",
    "top_n = 15\n",
    "print(f\"\\nTop {top_n} Most Important Features:\")\n",
    "for idx, row in feature_importance_df.head(top_n).iterrows():\n",
    "    bar_length = int(abs(row['abs_coefficient']) * 50)\n",
    "    bar = 'â–ˆ' * bar_length\n",
    "    print(f\"{row['feature']:25s} | {bar} {row['abs_coefficient']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7xsyejo39ii",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: REMOVE NOISY/UNIMPORTANT FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 2: Feature Selection - Remove Noisy Features\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Strategy 1: Remove features with near-zero coefficients\n",
    "threshold = 0.05  # Remove features with abs coefficient < threshold\n",
    "\n",
    "important_features_mask = abs_coefficients >= threshold\n",
    "n_features_removed = np.sum(~important_features_mask)\n",
    "n_features_kept = np.sum(important_features_mask)\n",
    "\n",
    "print(f\"\\nThreshold for feature selection: {threshold}\")\n",
    "print(f\"Features removed: {n_features_removed}\")\n",
    "print(f\"Features kept: {n_features_kept}\")\n",
    "print(f\"Reduction: {100 * n_features_removed / len(feature_names):.1f}%\")\n",
    "\n",
    "if n_features_removed > 0:\n",
    "    print(f\"\\nRemoved features:\")\n",
    "    removed_features = feature_importance_df[feature_importance_df['abs_coefficient'] < threshold]\n",
    "    for idx, row in removed_features.iterrows():\n",
    "        print(f\"  - {row['feature']:25s} (coef: {row['coefficient']:7.4f})\")\n",
    "    \n",
    "    print(f\"\\nKept features:\")\n",
    "    kept_features = feature_importance_df[feature_importance_df['abs_coefficient'] >= threshold]\n",
    "    for idx, row in kept_features.iterrows():\n",
    "        print(f\"  - {row['feature']:25s} (coef: {row['coefficient']:7.4f})\")\n",
    "\n",
    "# Create reduced feature sets\n",
    "X_v2_reduced = X_v2_scaled[:, important_features_mask]\n",
    "X_test_v2_reduced = X_test_v2_scaled[:, important_features_mask]\n",
    "\n",
    "print(f\"\\nOriginal feature shape: {X_v2_scaled.shape}\")\n",
    "print(f\"Reduced feature shape: {X_v2_reduced.shape}\")\n",
    "\n",
    "# Strategy 2: Also try selecting top K features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Alternative: Select Top K Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for top_k in [10, 15, 20, 25]:\n",
    "    if top_k <= len(feature_names):\n",
    "        print(f\"\\nTop {top_k} features would be:\")\n",
    "        for idx, row in feature_importance_df.head(top_k).iterrows():\n",
    "            print(f\"  {row['feature']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y6dybmqrxed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: RETRAIN MODELS WITH REDUCED FEATURE SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 3: Retrain Models with Reduced Feature Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train Logistic Regression with reduced features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Logistic Regression (Reduced Features)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clf_reduced = LogisticRegressionCV(\n",
    "    Cs=10,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf_reduced.fit(X_v2_reduced, y)\n",
    "\n",
    "test_probs_reduced = clf_reduced.predict_proba(X_test_v2_reduced)[:,1]\n",
    "test_preds_reduced = clf_reduced.predict(X_test_v2_reduced)\n",
    "\n",
    "print(f\"Best C: {clf_reduced.C_[0]:.4f}\")\n",
    "print(f\"CV AUC: {clf_reduced.scores_[1].mean(axis=0).max():.4f}\")\n",
    "\n",
    "print(f\"\\nTest Performance (Reduced Features):\")\n",
    "print(classification_report(y_test, test_preds_reduced))\n",
    "test_auc_reduced = roc_auc_score(y_test, test_probs_reduced)\n",
    "print(f\"Test AUC: {test_auc_reduced:.4f}\")\n",
    "\n",
    "# Train XGBoost with reduced features\n",
    "if test_probs_xgb is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"XGBoost (Reduced Features)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    xgb_model_reduced = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='auc'\n",
    "    )\n",
    "    \n",
    "    xgb_model_reduced.fit(X_v2_reduced, y)\n",
    "    test_probs_xgb_reduced = xgb_model_reduced.predict_proba(X_test_v2_reduced)[:,1]\n",
    "    test_auc_xgb_reduced = roc_auc_score(y_test, test_probs_xgb_reduced)\n",
    "    \n",
    "    print(f\"Test AUC: {test_auc_xgb_reduced:.4f}\")\n",
    "else:\n",
    "    test_auc_xgb_reduced = None\n",
    "\n",
    "# Train ensemble with reduced features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ensemble (Reduced Features)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ensemble_reduced_models = [\n",
    "    ('logistic', LogisticRegression(C=clf_reduced.C_[0], max_iter=1000, class_weight='balanced', random_state=42)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=8, min_samples_leaf=10,\n",
    "                                   class_weight='balanced', random_state=42, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=150, max_depth=4, learning_rate=0.05,\n",
    "                                       subsample=0.8, random_state=42))\n",
    "]\n",
    "\n",
    "if test_probs_xgb is not None:\n",
    "    ensemble_reduced_models.append(('xgb', xgb_model_reduced))\n",
    "\n",
    "ensemble_reduced = VotingClassifier(\n",
    "    estimators=ensemble_reduced_models,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble_reduced.fit(X_v2_reduced, y)\n",
    "test_probs_ensemble_reduced = ensemble_reduced.predict_proba(X_test_v2_reduced)[:,1]\n",
    "test_auc_ensemble_reduced = roc_auc_score(y_test, test_probs_ensemble_reduced)\n",
    "\n",
    "print(f\"Test AUC: {test_auc_ensemble_reduced:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lzk0ko5h5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: COMPARE FULL vs REDUCED FEATURE SETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE SELECTION RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# Logistic Regression\n",
    "comparison_data.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Full Features': test_auc_full,\n",
    "    'Reduced Features': test_auc_reduced,\n",
    "    'Difference': test_auc_reduced - test_auc_full,\n",
    "    'Improvement %': 100 * (test_auc_reduced - test_auc_full) / test_auc_full\n",
    "})\n",
    "\n",
    "# XGBoost\n",
    "if test_auc_xgb is not None and test_auc_xgb_reduced is not None:\n",
    "    comparison_data.append({\n",
    "        'Model': 'XGBoost',\n",
    "        'Full Features': test_auc_xgb,\n",
    "        'Reduced Features': test_auc_xgb_reduced,\n",
    "        'Difference': test_auc_xgb_reduced - test_auc_xgb,\n",
    "        'Improvement %': 100 * (test_auc_xgb_reduced - test_auc_xgb) / test_auc_xgb\n",
    "    })\n",
    "\n",
    "# Ensemble\n",
    "comparison_data.append({\n",
    "    'Model': 'Ensemble',\n",
    "    'Full Features': test_auc_ensemble,\n",
    "    'Reduced Features': test_auc_ensemble_reduced,\n",
    "    'Difference': test_auc_ensemble_reduced - test_auc_ensemble,\n",
    "    'Improvement %': 100 * (test_auc_ensemble_reduced - test_auc_ensemble) / test_auc_ensemble\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test AUC Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nNumber of features:\")\n",
    "print(f\"  Full feature set: {X_v2.shape[1]}\")\n",
    "print(f\"  Reduced feature set: {X_v2_reduced.shape[1]}\")\n",
    "print(f\"  Reduction: {100 * (X_v2.shape[1] - X_v2_reduced.shape[1]) / X_v2.shape[1]:.1f}%\")\n",
    "\n",
    "# Determine if feature selection helped\n",
    "best_full_auc = max(test_auc_full, test_auc_ensemble, test_auc_xgb if test_auc_xgb is not None else 0)\n",
    "best_reduced_auc = max(test_auc_reduced, test_auc_ensemble_reduced, test_auc_xgb_reduced if test_auc_xgb_reduced is not None else 0)\n",
    "\n",
    "print(f\"\\nBest performance:\")\n",
    "print(f\"  With all features: {best_full_auc:.4f}\")\n",
    "print(f\"  With reduced features: {best_reduced_auc:.4f}\")\n",
    "\n",
    "if best_reduced_auc > best_full_auc:\n",
    "    print(f\"\\nâœ… Feature selection IMPROVED performance by {100*(best_reduced_auc - best_full_auc)/best_full_auc:.2f}%\")\n",
    "    print(\"   Removing noisy features helped generalization!\")\n",
    "elif abs(best_reduced_auc - best_full_auc) < 0.001:\n",
    "    print(f\"\\nâž¡ï¸  Feature selection maintained performance ({abs(best_reduced_auc - best_full_auc):.4f} difference)\")\n",
    "    print(f\"   Achieved similar results with {100 * (X_v2.shape[1] - X_v2_reduced.shape[1]) / X_v2.shape[1]:.0f}% fewer features!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Feature selection DECREASED performance by {100*(best_full_auc - best_reduced_auc)/best_full_auc:.2f}%\")\n",
    "    print(\"   All features contribute to generalization in this case.\")\n",
    "\n",
    "# Save best submission from feature selection experiment\n",
    "if best_reduced_auc >= best_full_auc:\n",
    "    # Use reduced features\n",
    "    best_reduced_probs = test_probs_ensemble_reduced if test_auc_ensemble_reduced == best_reduced_auc else test_probs_reduced\n",
    "    submission_fs = test[[\"row_id\"]].copy()\n",
    "    submission_fs[\"prediction\"] = best_reduced_probs\n",
    "    submission_fs.to_csv(\"submission_feature_selected.csv\", index=False)\n",
    "    print(f\"\\nðŸ’¾ Saved submission_feature_selected.csv with reduced features (AUC: {best_reduced_auc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bycvoqfkr2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BONUS: REGULARIZATION STRENGTH ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORING DIFFERENT REGULARIZATION STRENGTHS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Try different C values (inverse of regularization strength)\n",
    "# Smaller C = stronger regularization = more features set to 0\n",
    "C_values = [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "regularization_results = []\n",
    "\n",
    "for C_val in C_values:\n",
    "    clf_temp = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        C=C_val,\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    clf_temp.fit(X_v2_scaled, y)\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    coeffs = clf_temp.coef_[0]\n",
    "    n_nonzero = np.sum(np.abs(coeffs) > 0.01)  # Count features with |coef| > 0.01\n",
    "    n_zero = len(coeffs) - n_nonzero\n",
    "    \n",
    "    # Evaluate performance\n",
    "    test_probs_temp = clf_temp.predict_proba(X_test_v2_scaled)[:,1]\n",
    "    test_auc_temp = roc_auc_score(y_test, test_probs_temp)\n",
    "    \n",
    "    regularization_results.append({\n",
    "        'C': C_val,\n",
    "        'Regularization Strength (1/C)': 1/C_val,\n",
    "        'Non-zero Features': n_nonzero,\n",
    "        'Zero Features': n_zero,\n",
    "        'Test AUC': test_auc_temp\n",
    "    })\n",
    "\n",
    "reg_df = pd.DataFrame(regularization_results)\n",
    "\n",
    "print(\"\\nRegularization Strength vs. Feature Count vs. Performance:\")\n",
    "print(reg_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Key Insights:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_auc_idx = reg_df['Test AUC'].idxmax()\n",
    "best_row = reg_df.iloc[best_auc_idx]\n",
    "\n",
    "print(f\"\\nBest performance:\")\n",
    "print(f\"  C = {best_row['C']}\")\n",
    "print(f\"  Features used: {best_row['Non-zero Features']}/{len(feature_names)}\")\n",
    "print(f\"  Test AUC: {best_row['Test AUC']:.4f}\")\n",
    "\n",
    "# Find the most parsimonious model (fewest features with good performance)\n",
    "# Define \"good performance\" as within 0.5% of best AUC\n",
    "auc_threshold = best_row['Test AUC'] * 0.995\n",
    "parsimonious_candidates = reg_df[reg_df['Test AUC'] >= auc_threshold]\n",
    "if len(parsimonious_candidates) > 0:\n",
    "    parsimonious_idx = parsimonious_candidates['Non-zero Features'].idxmin()\n",
    "    parsimonious_row = reg_df.iloc[parsimonious_idx]\n",
    "    \n",
    "    print(f\"\\nMost parsimonious model (within 0.5% of best AUC):\")\n",
    "    print(f\"  C = {parsimonious_row['C']}\")\n",
    "    print(f\"  Features used: {parsimonious_row['Non-zero Features']}/{len(feature_names)}\")\n",
    "    print(f\"  Test AUC: {parsimonious_row['Test AUC']:.4f}\")\n",
    "    print(f\"  Reduction: {100 * (len(feature_names) - parsimonious_row['Non-zero Features']) / len(feature_names):.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Recommendations:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. Balance between complexity and performance:\")\n",
    "print(\"   - Too few features: Underfitting, poor generalization\")\n",
    "print(\"   - Too many features: Overfitting, noisy predictions\")\n",
    "print(f\"\\n2. For this dataset, using ~{best_row['Non-zero Features']:.0f}-{parsimonious_row['Non-zero Features']:.0f} features\")\n",
    "print(\"   achieves optimal trade-off between model complexity and performance.\")\n",
    "print(\"\\n3. The most important features identified by L1 regularization are:\")\n",
    "print(\"   those with consistently high coefficients across different C values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y2i2063sfq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IDENTIFY THE 14 SELECTED FEATURES AT OPTIMAL C=0.05\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMAL FEATURE SET ANALYSIS (C=0.05)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train model with optimal C\n",
    "optimal_C = 0.05\n",
    "clf_optimal = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=optimal_C,\n",
    "    solver='liblinear',\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf_optimal.fit(X_v2_scaled, y)\n",
    "\n",
    "# Get coefficients\n",
    "optimal_coeffs = clf_optimal.coef_[0]\n",
    "optimal_abs_coeffs = np.abs(optimal_coeffs)\n",
    "\n",
    "# Create detailed feature analysis\n",
    "optimal_feature_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': optimal_coeffs,\n",
    "    'abs_coefficient': optimal_abs_coeffs,\n",
    "    'selected': optimal_abs_coeffs > 0.01  # Threshold for \"selected\"\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "# Separate selected and removed features\n",
    "selected_features = optimal_feature_df[optimal_feature_df['selected']]\n",
    "removed_features = optimal_feature_df[~optimal_feature_df['selected']]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SELECTED FEATURES ({len(selected_features)} features)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nThese features CONTRIBUTE to detecting rule violations:\\n\")\n",
    "\n",
    "for idx, row in selected_features.iterrows():\n",
    "    sign = \"+\" if row['coefficient'] > 0 else \"-\"\n",
    "    bar_length = int(row['abs_coefficient'] * 50)\n",
    "    bar = 'â–ˆ' * bar_length\n",
    "    \n",
    "    # Add interpretation\n",
    "    if row['coefficient'] > 0:\n",
    "        effect = \"â†’ INCREASES violation probability\"\n",
    "    else:\n",
    "        effect = \"â†’ DECREASES violation probability\"\n",
    "    \n",
    "    print(f\"{row['feature']:25s} | {sign} | {bar} {row['abs_coefficient']:.4f} | {effect}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"REMOVED FEATURES ({len(removed_features)} features)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nThese features were NOISY and hurt generalization:\\n\")\n",
    "\n",
    "for idx, row in removed_features.iterrows():\n",
    "    print(f\"  - {row['feature']:25s} (coef: {row['coefficient']:7.4f})\")\n",
    "\n",
    "# Performance with optimal features\n",
    "X_v2_optimal = X_v2_scaled[:, optimal_feature_df['selected'].values]\n",
    "X_test_v2_optimal = X_test_v2_scaled[:, optimal_feature_df['selected'].values]\n",
    "\n",
    "test_probs_optimal = clf_optimal.predict_proba(X_test_v2_scaled)[:,1]\n",
    "test_auc_optimal = roc_auc_score(y_test, test_probs_optimal)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PERFORMANCE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nTest AUC with optimal features: {test_auc_optimal:.4f}\")\n",
    "print(f\"Previous best (Ensemble, 35 features): {test_auc_ensemble:.4f}\")\n",
    "print(f\"Improvement: {test_auc_optimal - test_auc_ensemble:.4f} ({100*(test_auc_optimal - test_auc_ensemble)/test_auc_ensemble:.2f}%)\")\n",
    "\n",
    "# Save optimal submission\n",
    "submission_optimal = test[[\"row_id\"]].copy()\n",
    "submission_optimal[\"prediction\"] = test_probs_optimal\n",
    "submission_optimal.to_csv(\"submission_optimal_l1.csv\", index=False)\n",
    "print(f\"\\nâœ… Saved submission_optimal_l1.csv with 14 selected features!\")\n",
    "\n",
    "# Print feature categories breakdown\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FEATURE CATEGORY BREAKDOWN\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "categories = {\n",
    "    'Similarity Features': ['sim_pos1', 'sim_pos2', 'sim_neg1', 'sim_neg2', 'sim_rule'],\n",
    "    'Aggregate Features': ['pos_avg', 'neg_avg', 'pos_max', 'neg_max', 'pos_min', 'neg_min'],\n",
    "    'Ratio/Difference': ['pos_neg_ratio', 'pos_neg_diff', 'pos_range', 'neg_range'],\n",
    "    'Rule-Example Similarity': ['rule_pos_avg_sim', 'rule_neg_avg_sim'],\n",
    "    'Cross-Similarity': ['pos1_pos2_sim', 'neg1_neg2_sim'],\n",
    "    'Distance Features': ['dist_to_pos', 'dist_to_neg', 'dist_ratio'],\n",
    "    'Text Statistics': ['num_chars', 'num_words', 'has_url', 'caps_ratio'],\n",
    "    'Categorical': ['subreddit_encoded']\n",
    "}\n",
    "\n",
    "selected_names = set(selected_features['feature'].values)\n",
    "\n",
    "print(\"\\nWhich feature types were most valuable?\\n\")\n",
    "for category, feats in categories.items():\n",
    "    selected_count = sum(1 for f in feats if f in selected_names)\n",
    "    total_count = len(feats)\n",
    "    pct = 100 * selected_count / total_count if total_count > 0 else 0\n",
    "    \n",
    "    status = \"âœ“\" if selected_count > 0 else \"âœ—\"\n",
    "    print(f\"{status} {category:25s}: {selected_count}/{total_count} selected ({pct:.0f}%)\")\n",
    "    \n",
    "    # Show which specific features from this category were selected\n",
    "    if selected_count > 0:\n",
    "        selected_from_cat = [f for f in feats if f in selected_names]\n",
    "        for feat in selected_from_cat:\n",
    "            coef = selected_features[selected_features['feature'] == feat]['coefficient'].values[0]\n",
    "            print(f\"    â†’ {feat:25s} (coef: {coef:+.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2440ff5253549d0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
