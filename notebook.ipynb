{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:57:14.545251Z",
     "start_time": "2025-12-01T01:57:14.541809Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import platform\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587549a9-d222-482f-a390-1cb029529586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  1 05:43:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5090        On  |   00000000:42:00.0 Off |                  N/A |\n",
      "| 37%   45C    P8             14W /  575W |       0MiB /  32607MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b641d6",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e78c3653505758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:34.254561Z",
     "start_time": "2025-12-01T01:06:34.252850Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rule_col = \"rule\"\n",
    "body_col = \"body\"  \n",
    "label_col = \"rule_violation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547d82fe761b8357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:34.577067Z",
     "start_time": "2025-12-01T01:06:34.258530Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")  \n",
    "solution_df = pd.read_csv(\"data/solution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc549e0c8adc66da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:34.604223Z",
     "start_time": "2025-12-01T01:06:34.586985Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2029 entries, 0 to 2028\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   row_id              2029 non-null   int64 \n",
      " 1   body                2029 non-null   object\n",
      " 2   rule                2029 non-null   object\n",
      " 3   subreddit           2029 non-null   object\n",
      " 4   positive_example_1  2029 non-null   object\n",
      " 5   positive_example_2  2029 non-null   object\n",
      " 6   negative_example_1  2029 non-null   object\n",
      " 7   negative_example_2  2029 non-null   object\n",
      " 8   rule_violation      2029 non-null   int64 \n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 142.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>body</th>\n",
       "      <th>rule</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>negative_example_2</th>\n",
       "      <th>rule_violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Banks don't want you to know this! Click here ...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>Futurology</td>\n",
       "      <td>If you could tell your younger self something ...</td>\n",
       "      <td>hunt for lady for jack off in neighbourhood ht...</td>\n",
       "      <td>Watch Golden Globe Awards 2017 Live Online in ...</td>\n",
       "      <td>DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SD Stream [ ENG Link 1] (http://www.sportsstre...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>soccerstreams</td>\n",
       "      <td>[I wanna kiss you all over! Stunning!](http://...</td>\n",
       "      <td>LOLGA.COM is One of the First Professional Onl...</td>\n",
       "      <td>#Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...</td>\n",
       "      <td>[15 Amazing Hidden Features Of Google Search Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lol. Try appealing the ban and say you won't d...</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Don't break up with him or call the cops.  If ...</td>\n",
       "      <td>It'll be dismissed: https://en.wikipedia.org/w...</td>\n",
       "      <td>Where is there a site that still works where y...</td>\n",
       "      <td>Because this statement of his is true. It isn'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she will come your home open her legs with  an...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>sex</td>\n",
       "      <td>Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...</td>\n",
       "      <td>tight pussy watch for your cock get her at thi...</td>\n",
       "      <td>NSFW(obviously) http://spankbang.com/iy3u/vide...</td>\n",
       "      <td>Good News ::Download WhatsApp 2.16.230 APK for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>hearthstone</td>\n",
       "      <td>wow!! amazing reminds me of the old days.Well...</td>\n",
       "      <td>seek for lady for sex in around http://p77.pl/...</td>\n",
       "      <td>must be watch movie https://sites.google.com/s...</td>\n",
       "      <td>We're streaming Pokemon Veitnamese Crystal RIG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                                               body  \\\n",
       "0       0  Banks don't want you to know this! Click here ...   \n",
       "1       1  SD Stream [ ENG Link 1] (http://www.sportsstre...   \n",
       "2       2  Lol. Try appealing the ban and say you won't d...   \n",
       "3       3  she will come your home open her legs with  an...   \n",
       "4       4  code free tyrande --->>> [Imgur](http://i.imgu...   \n",
       "\n",
       "                                                rule      subreddit  \\\n",
       "0  No Advertising: Spam, referral links, unsolici...     Futurology   \n",
       "1  No Advertising: Spam, referral links, unsolici...  soccerstreams   \n",
       "2  No legal advice: Do not offer or request legal...   pcmasterrace   \n",
       "3  No Advertising: Spam, referral links, unsolici...            sex   \n",
       "4  No Advertising: Spam, referral links, unsolici...    hearthstone   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  If you could tell your younger self something ...   \n",
       "1  [I wanna kiss you all over! Stunning!](http://...   \n",
       "2  Don't break up with him or call the cops.  If ...   \n",
       "3  Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...   \n",
       "4   wow!! amazing reminds me of the old days.Well...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  hunt for lady for jack off in neighbourhood ht...   \n",
       "1  LOLGA.COM is One of the First Professional Onl...   \n",
       "2  It'll be dismissed: https://en.wikipedia.org/w...   \n",
       "3  tight pussy watch for your cock get her at thi...   \n",
       "4  seek for lady for sex in around http://p77.pl/...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  Watch Golden Globe Awards 2017 Live Online in ...   \n",
       "1  #Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...   \n",
       "2  Where is there a site that still works where y...   \n",
       "3  NSFW(obviously) http://spankbang.com/iy3u/vide...   \n",
       "4  must be watch movie https://sites.google.com/s...   \n",
       "\n",
       "                                  negative_example_2  rule_violation  \n",
       "0  DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...               0  \n",
       "1  [15 Amazing Hidden Features Of Google Search Y...               0  \n",
       "2  Because this statement of his is true. It isn'...               1  \n",
       "3  Good News ::Download WhatsApp 2.16.230 APK for...               1  \n",
       "4  We're streaming Pokemon Veitnamese Crystal RIG...               1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88acfceb9dd268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:34.645840Z",
     "start_time": "2025-12-01T01:06:34.626849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54059 entries, 0 to 54058\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   row_id              54059 non-null  int64 \n",
      " 1   body                54059 non-null  object\n",
      " 2   rule                54059 non-null  object\n",
      " 3   subreddit           54059 non-null  object\n",
      " 4   positive_example_1  54059 non-null  object\n",
      " 5   positive_example_2  54059 non-null  object\n",
      " 6   negative_example_1  54059 non-null  object\n",
      " 7   negative_example_2  54059 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 3.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>body</th>\n",
       "      <th>rule</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>negative_example_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2029</td>\n",
       "      <td>Hi Ken l! /u/Whimsical_wispers told me he want...</td>\n",
       "      <td>No promotion of illegal activity: Do not encou...</td>\n",
       "      <td>IAmA</td>\n",
       "      <td>There are several web sites on internet that a...</td>\n",
       "      <td>Someone needs to rape that judge and see how w...</td>\n",
       "      <td>call whores watch for you herein http://onj.me...</td>\n",
       "      <td>52 http://MySexFind.com - find girl for sex no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2030</td>\n",
       "      <td>Doubt anyone will read this but anti depressan...</td>\n",
       "      <td>No medical advice: Do not offer or request spe...</td>\n",
       "      <td>science</td>\n",
       "      <td>Maybe he just has alzheimer's. It would explai...</td>\n",
       "      <td>Hmm well I WAS going to ask women of Reddit ho...</td>\n",
       "      <td>You could always just carry your kid and get t...</td>\n",
       "      <td>Maybe you should get started on the solution a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2031</td>\n",
       "      <td>**[One of her best show](http://shortora.com/i...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>NSFW_GIF</td>\n",
       "      <td>Son Dakika Ek KazanÃ§ VideolarÄ±mÄ±zÄ± Ä°zleyin :\\n...</td>\n",
       "      <td>Try My New Game..! :)\\nhttps://play.google.com...</td>\n",
       "      <td>**HD** Stream[English Tiwtch](http://www.zifoo...</td>\n",
       "      <td>/r/krat0m &lt;-subscribe for free kilo of strong ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2032</td>\n",
       "      <td>If he'd cheat on you he'll cheat on her. She's...</td>\n",
       "      <td>No spoilers: Do not reveal important details t...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>And now Jessica is gone.  They really need to ...</td>\n",
       "      <td>Probably a meme at this point, particularly th...</td>\n",
       "      <td>hahah. Please, Barry changed his name to hide ...</td>\n",
       "      <td>I found the full video.\\nThere were a few peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2033</td>\n",
       "      <td>I think I remember reading somewhere that tech...</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>I mean, even if they didn't have anything sign...</td>\n",
       "      <td>Fire her. You can't be sued for it, only the c...</td>\n",
       "      <td>Personally. I'd be wrong to tell you suicide i...</td>\n",
       "      <td>I call it salary, perhaps a better term is sch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                                               body  \\\n",
       "0    2029  Hi Ken l! /u/Whimsical_wispers told me he want...   \n",
       "1    2030  Doubt anyone will read this but anti depressan...   \n",
       "2    2031  **[One of her best show](http://shortora.com/i...   \n",
       "3    2032  If he'd cheat on you he'll cheat on her. She's...   \n",
       "4    2033  I think I remember reading somewhere that tech...   \n",
       "\n",
       "                                                rule      subreddit  \\\n",
       "0  No promotion of illegal activity: Do not encou...           IAmA   \n",
       "1  No medical advice: Do not offer or request spe...        science   \n",
       "2  No Advertising: Spam, referral links, unsolici...       NSFW_GIF   \n",
       "3  No spoilers: Do not reveal important details t...  relationships   \n",
       "4  No legal advice: Do not offer or request legal...      AskReddit   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  There are several web sites on internet that a...   \n",
       "1  Maybe he just has alzheimer's. It would explai...   \n",
       "2  Son Dakika Ek KazanÃ§ VideolarÄ±mÄ±zÄ± Ä°zleyin :\\n...   \n",
       "3  And now Jessica is gone.  They really need to ...   \n",
       "4  I mean, even if they didn't have anything sign...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  Someone needs to rape that judge and see how w...   \n",
       "1  Hmm well I WAS going to ask women of Reddit ho...   \n",
       "2  Try My New Game..! :)\\nhttps://play.google.com...   \n",
       "3  Probably a meme at this point, particularly th...   \n",
       "4  Fire her. You can't be sued for it, only the c...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  call whores watch for you herein http://onj.me...   \n",
       "1  You could always just carry your kid and get t...   \n",
       "2  **HD** Stream[English Tiwtch](http://www.zifoo...   \n",
       "3  hahah. Please, Barry changed his name to hide ...   \n",
       "4  Personally. I'd be wrong to tell you suicide i...   \n",
       "\n",
       "                                  negative_example_2  \n",
       "0  52 http://MySexFind.com - find girl for sex no...  \n",
       "1  Maybe you should get started on the solution a...  \n",
       "2  /r/krat0m <-subscribe for free kilo of strong ...  \n",
       "3  I found the full video.\\nThere were a few peop...  \n",
       "4  I call it salary, perhaps a better term is sch...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.info()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ed73246eb091d56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:01:50.841264Z",
     "start_time": "2025-12-01T01:01:50.798991Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set\n",
      "Total samples: 2029\n",
      "Class distribution:\n",
      "rule_violation\n",
      "1    1031\n",
      "0     998\n",
      "Name: count, dtype: int64\n",
      "Class ratio: rule_violation\n",
      "1    0.508132\n",
      "0    0.491868\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Unique rules in train: 2\n",
      "Rules: ['No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.'\n",
      " 'No legal advice: Do not offer or request legal advice.']\n",
      "\n",
      "Unique subreddits in train: 100\n",
      "\n",
      "Test Set\n",
      "Total samples: 54059\n",
      "\n",
      "Unique rules in test: 6\n",
      "Rules: ['No promotion of illegal activity: Do not encourage or promote illegal activities, such as drug-related activity, violence, exploitation, theft, or other criminal behavior.'\n",
      " 'No medical advice: Do not offer or request specific medical advice, diagnoses, or treatment recommendations.'\n",
      " 'No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.'\n",
      " \"No spoilers: Do not reveal important details that would limit people's ability to enjoy a show or movie.\"\n",
      " 'No legal advice: Do not offer or request legal advice.'\n",
      " 'No financial advice: We do not permit comments that make personal recommendations for investments, taxes, or careers.']\n",
      "\n",
      "Unique subreddits in test: 100\n",
      "\n",
      "Analysis\n",
      "Rules in test but NOT in train: {'No medical advice: Do not offer or request specific medical advice, diagnoses, or treatment recommendations.', 'No financial advice: We do not permit comments that make personal recommendations for investments, taxes, or careers.', 'No promotion of illegal activity: Do not encourage or promote illegal activities, such as drug-related activity, violence, exploitation, theft, or other criminal behavior.', \"No spoilers: Do not reveal important details that would limit people's ability to enjoy a show or movie.\"}\n",
      "Number of test samples with unseen rules: 36088\n",
      "Percentage: 66.76%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining Set\")\n",
    "print(f\"Total samples: {len(train_df)}\")\n",
    "print(f\"Class distribution:\\n{train_df[label_col].value_counts()}\")\n",
    "print(f\"Class ratio: {train_df[label_col].value_counts(normalize=True)}\")\n",
    "print(f\"\\nUnique rules in train: {train_df[rule_col].nunique()}\")\n",
    "print(f\"Rules: {train_df[rule_col].unique()}\")\n",
    "print(f\"\\nUnique subreddits in train: {train_df['subreddit'].nunique()}\")\n",
    "\n",
    "print(\"\\nTest Set\")\n",
    "print(f\"Total samples: {len(test_df)}\")\n",
    "print(f\"\\nUnique rules in test: {test_df[rule_col].nunique()}\")\n",
    "print(f\"Rules: {test_df[rule_col].unique()}\")\n",
    "print(f\"\\nUnique subreddits in test: {test_df['subreddit'].nunique()}\")\n",
    "\n",
    "print(\"\\nAnalysis\")\n",
    "train_rules = set(train_df[rule_col].unique())\n",
    "test_rules = set(test_df[rule_col].unique())\n",
    "print(f\"Rules in test but NOT in train: {test_rules - train_rules}\")\n",
    "print(f\"Number of test samples with unseen rules: {test_df[~test_df[rule_col].isin(train_rules)].shape[0]}\")\n",
    "print(f\"Percentage: {100 * test_df[~test_df[rule_col].isin(train_rules)].shape[0] / len(test_df):.2f}%\")\n",
    "\n",
    "train_subreddits = set(train_df['subreddit'].unique())\n",
    "test_subreddits = set(test_df['subreddit'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3361ddcbd486d5",
   "metadata": {},
   "source": [
    "During our data analysis we noticed a few pain points in solving this problem. First, we have a small training dataset of 2,029 samples, and we are evaluating this on a test set of 54,059 samples. Secondly, we have an additional 4 rules in our test dataset that are unseen in our training dataset, these unseen rules make up 66.76% of our test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c47e0387c7bcd",
   "metadata": {},
   "source": [
    "# Training Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbcbb158-7345-4b4e-859b-bff0e0ac698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif platform.system() == \"Darwin\" and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f5655cab46450b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:39.993742Z",
     "start_time": "2025-12-01T01:06:38.269792Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"./e5-large-v2-triplet\", device=\"cuda\")\n",
    "\n",
    "def embed_batch(texts):\n",
    "    return model.encode(\n",
    "        texts,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b95f12a6fe4a8e32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:41.211668Z",
     "start_time": "2025-12-01T01:06:41.207563Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_or_create_embeddings(df, prefix, cache_dir=\"embeddings_cache_triplet\"):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    embeddings = {}\n",
    "\n",
    "    to_embed = {\n",
    "        'body': body_col,\n",
    "        'pos1': 'positive_example_1',\n",
    "        'pos2': 'positive_example_2',\n",
    "        'neg1': 'negative_example_1',\n",
    "        'neg2': 'negative_example_2'\n",
    "    }\n",
    "\n",
    "    for key, col in to_embed.items():\n",
    "        cache_file = f\"{cache_dir}/{prefix}_{key}_emb.pkl\"\n",
    "\n",
    "        if os.path.exists(cache_file):\n",
    "            print(f\"Loading cached {prefix} {key} embeddings...\")\n",
    "            embeddings[f\"{key}_emb\"] = pickle.load(open(cache_file, 'rb'))\n",
    "        else:\n",
    "            print(f\"Embedding {prefix} {col}...\")\n",
    "            embeddings[f\"{key}_emb\"] = embed_batch(df[col].tolist())\n",
    "            pickle.dump(embeddings[f\"{key}_emb\"], open(cache_file, 'wb'))\n",
    "\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "567fd704fe5596f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:42.643371Z",
     "start_time": "2025-12-01T01:06:42.640042Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_features(text):\n",
    "    num_chars = len(text)\n",
    "    num_words = len(text.split())\n",
    "\n",
    "    has_url = 1 if re.search(r'http[s]?://|www\\.', text) else 0\n",
    "\n",
    "    if num_chars > 0:\n",
    "        caps_ratio = sum(1 for c in text if c.isupper()) / num_chars\n",
    "    else:\n",
    "        caps_ratio = 0\n",
    "\n",
    "    return np.array([num_chars, num_words, has_url, caps_ratio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "668eab8304c3e523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:06:44.161731Z",
     "start_time": "2025-12-01T01:06:44.157444Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_features(body, rule, pos1, pos2, neg1, neg2, text, subreddit_encoded):\n",
    "    sim_pos1 = (body * pos1).sum()\n",
    "    sim_pos2 = (body * pos2).sum()\n",
    "    sim_neg1 = (body * neg1).sum()\n",
    "    sim_neg2 = (body * neg2).sum()\n",
    "    sim_rule = (body * rule).sum()\n",
    "\n",
    "    pos_avg = (sim_pos1 + sim_pos2) / 2\n",
    "    neg_avg = (sim_neg1 + sim_neg2) / 2\n",
    "    pos_neg_ratio = pos_avg / (neg_avg + 1e-6)\n",
    "    pos_neg_diff = pos_avg - neg_avg\n",
    "\n",
    "    text_feats = extract_text_features(text)\n",
    "\n",
    "    return np.concatenate([\n",
    "        [sim_pos1, sim_pos2, sim_neg1, sim_neg2, sim_rule],  # 5 features\n",
    "        [pos_neg_ratio, pos_neg_diff],  # 2 features\n",
    "        text_feats,  # 4 features (num_chars, num_words, has_url, caps_ratio)\n",
    "        [subreddit_encoded]  # 1 feature\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "571ff98694bbcb91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:07:21.415276Z",
     "start_time": "2025-12-01T01:07:20.341659Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding unique rules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5316ff935ed46e9b1d5e205086c5650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding train set\n",
      "Loading cached train body embeddings...\n",
      "Loading cached train pos1 embeddings...\n",
      "Loading cached train pos2 embeddings...\n",
      "Loading cached train neg1 embeddings...\n",
      "Loading cached train neg2 embeddings...\n",
      "\n",
      "Embedding test set\n",
      "Loading cached test body embeddings...\n",
      "Loading cached test pos1 embeddings...\n",
      "Loading cached test pos2 embeddings...\n",
      "Loading cached test neg1 embeddings...\n",
      "Loading cached test neg2 embeddings...\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding unique rules...\")\n",
    "all_unique_rules = pd.concat([train_df[rule_col], test_df[rule_col]]).unique()\n",
    "unique_rule_emb = embed_batch(all_unique_rules.tolist())\n",
    "rule_to_emb = dict(zip(all_unique_rules, unique_rule_emb))\n",
    "rule_emb = np.array([rule_to_emb[rule] for rule in train_df[rule_col]])\n",
    "\n",
    "print(\"\\nEmbedding train set\")\n",
    "train_emb = load_or_create_embeddings(train_df, prefix='train')\n",
    "body_emb = train_emb['body_emb']\n",
    "pos1_emb = train_emb['pos1_emb']\n",
    "pos2_emb = train_emb['pos2_emb']\n",
    "neg1_emb = train_emb['neg1_emb']\n",
    "neg2_emb = train_emb['neg2_emb']\n",
    "rule_emb_test = np.array([rule_to_emb[rule] for rule in test_df[rule_col]])\n",
    "\n",
    "print(\"\\nEmbedding test set\")\n",
    "test_emb = load_or_create_embeddings(test_df, prefix='test')\n",
    "body_emb_test = test_emb['body_emb']\n",
    "pos1_emb_test = test_emb['pos1_emb']\n",
    "pos2_emb_test = test_emb['pos2_emb']\n",
    "neg1_emb_test = test_emb['neg1_emb']\n",
    "neg2_emb_test = test_emb['neg2_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0d4a7933f3a9bbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:07:36.531238Z",
     "start_time": "2025-12-01T01:07:36.490491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LabelEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_encoder = LabelEncoder()\n",
    "subreddit_encoder.fit(pd.concat([train_df['subreddit'], test_df['subreddit']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff9792c583b459bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:07:37.946889Z",
     "start_time": "2025-12-01T01:07:37.881873Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (2029, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2029, 12), (2029,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subreddit_encoded = subreddit_encoder.transform(train_df['subreddit'])\n",
    "y = train_df[label_col].values\n",
    "\n",
    "X = np.array([\n",
    "    combine_features(\n",
    "        body_emb[i], rule_emb[i], \n",
    "        pos1_emb[i], pos2_emb[i], \n",
    "        neg1_emb[i], neg2_emb[i],\n",
    "        train_df[body_col].iloc[i],\n",
    "        train_subreddit_encoded[i]\n",
    "    ) \n",
    "    for i in range(len(train_df))\n",
    "])\n",
    "\n",
    "\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89373b0e9845959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:07:41.027919Z",
     "start_time": "2025-12-01T01:07:39.519997Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features shape: (54059, 12)\n"
     ]
    }
   ],
   "source": [
    "test_subreddit_encoded = subreddit_encoder.transform(test_df['subreddit'])\n",
    "\n",
    "X_test = np.array([\n",
    "    combine_features(\n",
    "        body_emb_test[i], rule_emb_test[i],\n",
    "        pos1_emb_test[i], pos2_emb_test[i],\n",
    "        neg1_emb_test[i], neg2_emb_test[i],\n",
    "        test_df[body_col].iloc[i],\n",
    "        test_subreddit_encoded[i]\n",
    "    )\n",
    "    for i in range(len(test_df))\n",
    "])\n",
    "\n",
    "print(f\"Test features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e45c05cc3fd51",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f743b989ed016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:06:01.251538Z",
     "start_time": "2025-12-01T02:05:52.206431Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, n_jobs=-1, class_weight=\"balanced\"),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=10, n_jobs=-1, class_weight=\"balanced\", random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=200, max_depth=4, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, reg_alpha=1.0, reg_lambda=1.0, random_state=42, n_jobs=-1, eval_metric='logloss'),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    test_preds = model.predict(X_test)\n",
    "    test_probs = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    test_results = test_df[[\"row_id\"]].copy()\n",
    "    test_results[\"prediction\"] = test_probs\n",
    "    test_results = test_results.merge(solution[[\"row_id\", \"rule_violation\"]], on=\"row_id\", how=\"left\")\n",
    "    y_test = test_results[\"rule_violation\"].values\n",
    "    \n",
    "    print(f\"Metrics on test dataset\")\n",
    "    print(classification_report(y_test, test_preds))\n",
    "    test_auc = roc_auc_score(y_test, test_probs)\n",
    "    test_acc = (test_preds == y_test).mean()\n",
    "    print(f\"AUC: {test_auc:.4f}\")\n",
    "    print(f\"Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    \n",
    "    results[name] = {\n",
    "        'test_auc': test_auc,\n",
    "        'test_acc': test_acc,\n",
    "        'model': model,\n",
    "        'test_probs': test_probs\n",
    "    }\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Summary\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:20s} | Test AUC: {res['test_auc']:.4f} | Test Acc: {res['test_acc']:.4f}\")\n",
    "\n",
    "# best_model_name = max(results.items(), key=lambda x: x[1]['test_auc'])[0]\n",
    "# best_probs = results[best_model_name]['test_probs']\n",
    "# test[[\"row_id\"]].assign(prediction=best_probs).to_csv(\"submission.csv\", index=False)\n",
    "# print(f\"\\nSaved submission.csv using {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "uhj559b43d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:08:29.351882Z",
     "start_time": "2025-12-01T02:08:29.335309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved neural network system loaded.\n",
      "Training Neural Network...\n",
      "Best Validation AUC during training: 0.9296\n",
      "\n",
      "Metrics on test dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75     28789\n",
      "           1       0.71      0.68      0.70     25270\n",
      "\n",
      "    accuracy                           0.72     54059\n",
      "   macro avg       0.72      0.72      0.72     54059\n",
      "weighted avg       0.72      0.72      0.72     54059\n",
      "\n",
      "AUC: 0.7707\n",
      "Accuracy: 0.7239\n",
      "\n",
      "\n",
      "Updated Summary (including Neural Network)\n",
      "Logistic Regression  | Test AUC: 0.7709 | Test Acc: 0.7201\n",
      "Random Forest        | Test AUC: 0.7653 | Test Acc: 0.6506\n",
      "Gradient Boosting    | Test AUC: 0.7416 | Test Acc: 0.6259\n",
      "XGBoost              | Test AUC: 0.7592 | Test Acc: 0.6425\n",
      "SVM (RBF)            | Test AUC: 0.7580 | Test Acc: 0.6950\n",
      "Neural Network       | Test AUC: 0.7707 | Test Acc: 0.7239\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# Global Seed (Better Reproducibility)\n",
    "# ==========================================\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ==========================================\n",
    "# Improved Architecture with Residual Blocks\n",
    "# ==========================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)  # skip connection\n",
    "\n",
    "\n",
    "class RuleViolationNet(nn.Module):\n",
    "    def __init__(self, input_dim=12, hidden_dims=[64, 32, 16], dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers += [\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout),\n",
    "\n",
    "                # add a residual block for each hidden dimension\n",
    "                ResidualBlock(hidden_dim, dropout)\n",
    "            ]\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        # Note: NO Sigmoid here â€” BCEWithLogitsLoss is better\n",
    "        self.output_layer = nn.Linear(prev_dim, 1)\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return self.output_layer(x)  # logits (not sigmoid)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Dataset Wrapper\n",
    "# ==========================================\n",
    "\n",
    "class RuleViolationDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = None\n",
    "        if y is not None:\n",
    "            self.y = torch.FloatTensor(y).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Neural Network Training\n",
    "# ==========================================\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "def train_nn_fixed(X, y, groups, epochs=80, batch_size=32):\n",
    "\n",
    "    gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
    "    train_idx, val_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # scaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    train_ds = RuleViolationDataset(X_train, y_train)\n",
    "    val_ds   = RuleViolationDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    # simple model\n",
    "    model = RuleViolationNet(input_dim=X.shape[1], hidden_dims=[64, 32], dropout=0.4).to(device)\n",
    "\n",
    "    # class balancing\n",
    "    pos_weight = torch.tensor([(y_train.mean() / (1 - y_train.mean()))]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.003,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    best_auc = 0\n",
    "    patience = 12\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        preds, labs = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb)\n",
    "                preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                labs.extend(yb.numpy())\n",
    "\n",
    "        auc = roc_auc_score(labs, preds)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, scaler, best_auc\n",
    "\n",
    "# ==========================================\n",
    "# Prediction\n",
    "# ==========================================\n",
    "\n",
    "def predict_neural_network(model, scaler, X_test, batch_size=32):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    test_dataset = RuleViolationDataset(X_test_scaled)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            logits = model(batch_X)\n",
    "            preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "\n",
    "    return np.array(preds).flatten()\n",
    "\n",
    "print(\"Improved neural network system loaded.\")\n",
    "print(\"Training Neural Network...\")\n",
    "\n",
    "# You need grouping to avoid leakage:\n",
    "groups = train[\"subreddit\"].astype(str).values   # or \"rule\"\n",
    "\n",
    "# Train model using improved fixed NN trainer\n",
    "nn_model, nn_scaler, best_val_auc = train_nn_fixed(\n",
    "    X, \n",
    "    y, \n",
    "    groups,\n",
    "    epochs=100,\n",
    ")\n",
    "\n",
    "print(f\"Best Validation AUC during training: {best_val_auc:.4f}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_probs_nn = predict_neural_network(nn_model, nn_scaler, X_test)\n",
    "test_preds_nn = (test_probs_nn > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "test_results_nn = test[[\"row_id\"]].copy()\n",
    "test_results_nn[\"prediction\"] = test_probs_nn\n",
    "test_results_nn = test_results_nn.merge(\n",
    "    solution[[\"row_id\", \"rule_violation\"]],\n",
    "    on=\"row_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "y_test_nn = test_results_nn[\"rule_violation\"].values\n",
    "\n",
    "print(\"\\nMetrics on test dataset\")\n",
    "print(classification_report(y_test_nn, test_preds_nn))\n",
    "\n",
    "test_auc_nn = roc_auc_score(y_test_nn, test_probs_nn)\n",
    "test_acc_nn = (test_preds_nn == y_test_nn).mean()\n",
    "\n",
    "print(f\"AUC: {test_auc_nn:.4f}\")\n",
    "print(f\"Accuracy: {test_acc_nn:.4f}\")\n",
    "\n",
    "# Add to results\n",
    "results[\"Neural Network\"] = {\n",
    "    \"test_auc\": test_auc_nn,\n",
    "    \"test_acc\": test_acc_nn,\n",
    "    \"model\": nn_model,\n",
    "    \"test_probs\": test_probs_nn,\n",
    "}\n",
    "\n",
    "print(\"\\n\\nUpdated Summary (including Neural Network)\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:20s} | Test AUC: {res['test_auc']:.4f} | Test Acc: {res['test_acc']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "i4h5kbm3fde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:17:16.632946Z",
     "start_time": "2025-12-01T02:17:14.177443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating full embedding features for training set...\n",
      "Original feature shape: (2029, 12)\n",
      "Full embedding feature shape: (2029, 6156)\n",
      "Total dimensions: 6156\n",
      "\n",
      "Creating full embedding features for test set...\n",
      "Test full embedding feature shape: (54059, 6156)\n"
     ]
    }
   ],
   "source": [
    "# Create full embedding features for training set\n",
    "print(\"Creating full embedding features for training set...\")\n",
    "\n",
    "# Concatenate all embeddings + engineered features\n",
    "X_full = np.concatenate([\n",
    "    body_emb,       # 768 dims\n",
    "    rule_emb,       # 768 dims  \n",
    "    pos1_emb,       # 768 dims\n",
    "    pos2_emb,       # 768 dims\n",
    "    neg1_emb,       # 768 dims\n",
    "    neg2_emb,       # 768 dims\n",
    "    X               # 12 engineered features (similarities, text features, subreddit)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Original feature shape: {X.shape}\")\n",
    "print(f\"Full embedding feature shape: {X_full.shape}\")\n",
    "print(f\"Total dimensions: {X_full.shape[1]}\")\n",
    "\n",
    "# Create full embedding features for test set\n",
    "print(\"\\nCreating full embedding features for test set...\")\n",
    "X_test_full = np.concatenate([\n",
    "    body_emb_test,\n",
    "    rule_emb_test,\n",
    "    pos1_emb_test,\n",
    "    pos2_emb_test,\n",
    "    neg1_emb_test,\n",
    "    neg2_emb_test,\n",
    "    X_test\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Test full embedding feature shape: {X_test_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "l5hko0pk0tn",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:18:50.235111Z",
     "start_time": "2025-12-01T02:18:07.355417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Improved Neural Network...\n",
      "Changes:\n",
      "  - Using full 4620-dimensional features (embeddings + engineered features)\n",
      "  - Simpler architecture: [256, 64] instead of [128, 64, 32]\n",
      "  - Higher dropout: 0.5 instead of 0.3 (better for small datasets)\n",
      "  - More regularization: weight_decay increased\n",
      "\n",
      "Epoch [10/100] - Train Loss: 0.1243 - Val Loss: 0.4307 - Val AUC: 0.9380\n",
      "Epoch [20/100] - Train Loss: 0.0796 - Val Loss: 0.4761 - Val AUC: 0.9433\n",
      "Epoch [30/100] - Train Loss: 0.0425 - Val Loss: 0.5416 - Val AUC: 0.9405\n",
      "Epoch [40/100] - Train Loss: 0.0201 - Val Loss: 0.5936 - Val AUC: 0.9387\n",
      "Epoch [50/100] - Train Loss: 0.0223 - Val Loss: 0.6573 - Val AUC: 0.9369\n",
      "Epoch [60/100] - Train Loss: 0.0276 - Val Loss: 0.7073 - Val AUC: 0.9337\n",
      "Epoch [70/100] - Train Loss: 0.0232 - Val Loss: 0.7505 - Val AUC: 0.9357\n",
      "Epoch [80/100] - Train Loss: 0.0106 - Val Loss: 0.8070 - Val AUC: 0.9365\n",
      "Epoch [90/100] - Train Loss: 0.0036 - Val Loss: 0.7916 - Val AUC: 0.9368\n",
      "Epoch [100/100] - Train Loss: 0.0141 - Val Loss: 0.7951 - Val AUC: 0.9361\n",
      "\n",
      "============================================================\n",
      "IMPROVED NEURAL NETWORK - Metrics on test dataset\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.68      0.62     28789\n",
      "           1       0.53      0.41      0.46     25270\n",
      "\n",
      "    accuracy                           0.55     54059\n",
      "   macro avg       0.55      0.54      0.54     54059\n",
      "weighted avg       0.55      0.55      0.54     54059\n",
      "\n",
      "AUC: 0.6196\n",
      "Accuracy: 0.5515\n",
      "\n",
      "============================================================\n",
      "COMPARISON: All Models\n",
      "============================================================\n",
      "Logistic Regression            | Test AUC: 0.7762 | Test Acc: 0.6764\n",
      "Random Forest                  | Test AUC: 0.7644 | Test Acc: 0.6627\n",
      "XGBoost                        | Test AUC: 0.7533 | Test Acc: 0.6580\n",
      "Neural Network                 | Test AUC: 0.7498 | Test Acc: 0.6317\n",
      "Gradient Boosting              | Test AUC: 0.7426 | Test Acc: 0.6539\n",
      "SVM (RBF)                      | Test AUC: 0.6381 | Test Acc: 0.5973\n",
      "Neural Network (Improved)      | Test AUC: 0.6196 | Test Acc: 0.5515\n",
      "\n",
      "============================================================\n",
      "Neural Network Improvement: -0.1302 AUC\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train Improved Neural Network with Full Embeddings + Simpler Architecture\n",
    "print(\"Training Improved Neural Network...\")\n",
    "print(\"Changes:\")\n",
    "print(\"  - Using full 4620-dimensional features (embeddings + engineered features)\")\n",
    "print(\"  - Simpler architecture: [256, 64] instead of [128, 64, 32]\")\n",
    "print(\"  - Higher dropout: 0.5 instead of 0.3 (better for small datasets)\")\n",
    "print(\"  - More regularization: weight_decay increased\\n\")\n",
    "\n",
    "# Split data for validation\n",
    "X_train_full, X_val_full, y_train_full, y_val_full = train_test_split(\n",
    "    X_full, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train improved model with simpler architecture\n",
    "nn_model_improved, nn_scaler_improved, history_improved = train_neural_network(\n",
    "    X_train_full, y_train_full,\n",
    "    X_val_full, y_val_full,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    hidden_dims=[256, 64],  # Simpler: 2 layers instead of 3\n",
    "    dropout=0.5              # More aggressive dropout for small dataset\n",
    ")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_probs_nn_improved = predict_neural_network(nn_model_improved, nn_scaler_improved, X_test_full)\n",
    "test_preds_nn_improved = (test_probs_nn_improved > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "test_results_nn_improved = test[[\"row_id\"]].copy()\n",
    "test_results_nn_improved[\"prediction\"] = test_probs_nn_improved\n",
    "test_results_nn_improved = test_results_nn_improved.merge(solution[[\"row_id\", \"rule_violation\"]], on=\"row_id\", how=\"left\")\n",
    "y_test_nn_improved = test_results_nn_improved[\"rule_violation\"].values\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVED NEURAL NETWORK - Metrics on test dataset\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test_nn_improved, test_preds_nn_improved))\n",
    "test_auc_nn_improved = roc_auc_score(y_test_nn_improved, test_probs_nn_improved)\n",
    "test_acc_nn_improved = (test_preds_nn_improved == y_test_nn_improved).mean()\n",
    "print(f\"AUC: {test_auc_nn_improved:.4f}\")\n",
    "print(f\"Accuracy: {test_acc_nn_improved:.4f}\")\n",
    "\n",
    "# Add to results\n",
    "results['Neural Network (Improved)'] = {\n",
    "    'test_auc': test_auc_nn_improved,\n",
    "    'test_acc': test_acc_nn_improved,\n",
    "    'model': nn_model_improved,\n",
    "    'test_probs': test_probs_nn_improved\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: All Models\")\n",
    "print(\"=\"*60)\n",
    "for name, res in sorted(results.items(), key=lambda x: x[1]['test_auc'], reverse=True):\n",
    "    print(f\"{name:30s} | Test AUC: {res['test_auc']:.4f} | Test Acc: {res['test_acc']:.4f}\")\n",
    "\n",
    "# Highlight improvement\n",
    "original_nn_auc = results['Neural Network']['test_auc']\n",
    "improvement = test_auc_nn_improved - original_nn_auc\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Neural Network Improvement: {improvement:+.4f} AUC\")\n",
    "if test_auc_nn_improved > original_nn_auc:\n",
    "    print(f\"âœ“ {abs(improvement)/original_nn_auc*100:.1f}% relative improvement!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7fffed43-c7bf-4c73-a79a-cd99c35a1eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "Loading triplet model...\n",
      "Embedding training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323707f05da242128e5864255c4a3ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0875fba7e8d44ce99d393f749c524f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1690 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embedding shape: (2029, 1024)\n",
      "Test embedding shape: (54059, 1024)\n",
      "Training Neural Network (Triplet Embeddings)...\n",
      "Best Validation AUC: 0.9443\n",
      "Predicting on test set...\n",
      "\n",
      "Final Test Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68     28789\n",
      "           1       0.63      0.46      0.53     25270\n",
      "\n",
      "    accuracy                           0.62     54059\n",
      "   macro avg       0.62      0.61      0.61     54059\n",
      "weighted avg       0.62      0.62      0.61     54059\n",
      "\n",
      "AUC: 0.6329\n",
      "Accuracy: 0.6214\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# FULL PIPELINE â€” Triplet Embeddings â†’ Neural Network Classifier\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ===============================\n",
    "# 1. Device selection\n",
    "# ===============================\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# ===============================\n",
    "# 2. Load Triplet-Tuned Embedding Model\n",
    "# ===============================\n",
    "print(\"Loading triplet model...\")\n",
    "model_triplet = SentenceTransformer(\"./e5-large-v2-triplet\", device=device)\n",
    "\n",
    "# ===============================\n",
    "# 3. Embed Training + Test Data\n",
    "# ===============================\n",
    "print(\"Embedding training data...\")\n",
    "X_full = model_triplet.encode(\n",
    "    train[\"body\"].tolist(),\n",
    "    batch_size=32,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Embedding test data...\")\n",
    "X_test_full = model_triplet.encode(\n",
    "    test[\"body\"].tolist(),\n",
    "    batch_size=32,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "y_full = train_df[\"rule_violation\"].astype(int).values\n",
    "groups = train_df[\"subreddit\"].astype(str).values  # Group split to avoid leakage\n",
    "\n",
    "print(\"Train embedding shape:\", X_full.shape)\n",
    "print(\"Test embedding shape:\", X_test_full.shape)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 4. Dataset & Model Definitions\n",
    "# ================================================================\n",
    "class RuleViolationDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = None if y is None else torch.FloatTensor(y).unsqueeze(1)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class RuleViolationNet(nn.Module):\n",
    "    def __init__(self, input_dim=1024, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5. Improved Neural Network Trainer\n",
    "# ================================================================\n",
    "def train_nn_fixed(X, y, groups, epochs=80, batch_size=32):\n",
    "\n",
    "    gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
    "    train_idx, val_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    train_ds = RuleViolationDataset(X_train, y_train)\n",
    "    val_ds = RuleViolationDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    model = RuleViolationNet(input_dim=X.shape[1]).to(device)\n",
    "\n",
    "    pos_weight = torch.tensor([(y_train.mean() / (1 - y_train.mean()))]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=0.003,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    best_auc = 0\n",
    "    patience = 12\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        preds, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb)\n",
    "                preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                labels.extend(yb.numpy())\n",
    "\n",
    "        auc = roc_auc_score(labels, preds)\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            no_improve = 0\n",
    "            best_state = model.state_dict()\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, scaler, best_auc\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6. Train Neural Network on Full Triplet Embeddings\n",
    "# ================================================================\n",
    "print(\"Training Neural Network (Triplet Embeddings)...\")\n",
    "nn_model, nn_scaler, best_val_auc = train_nn_fixed(X_full, y_full, groups, epochs=80)\n",
    "\n",
    "print(f\"Best Validation AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 7. Predict on Test Set\n",
    "# ================================================================\n",
    "def predict_nn(model, scaler, X):\n",
    "    X_scaled = scaler.transform(X)\n",
    "    ds = RuleViolationDataset(X_scaled)\n",
    "    loader = DataLoader(ds, batch_size=64)\n",
    "\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "    return np.array(preds).flatten()\n",
    "\n",
    "\n",
    "print(\"Predicting on test set...\")\n",
    "test_probs_nn = predict_nn(nn_model, nn_scaler, X_test_full)\n",
    "test_preds_nn = (test_probs_nn > 0.5).astype(int)\n",
    "\n",
    "# Evaluate on \"solution\"\n",
    "test_results = test[[\"row_id\"]].copy()\n",
    "test_results[\"prediction\"] = test_probs_nn\n",
    "test_results = test_results.merge(solution[[\"row_id\", \"rule_violation\"]], on=\"row_id\")\n",
    "y_test_true = test_results[\"rule_violation\"].values\n",
    "\n",
    "print(\"\\nFinal Test Metrics:\")\n",
    "print(classification_report(y_test_true, test_preds_nn))\n",
    "\n",
    "auc = roc_auc_score(y_test_true, test_probs_nn)\n",
    "acc = (test_preds_nn == y_test_true).mean()\n",
    "\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "w7m9ctrwux8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:28:28.514272Z",
     "start_time": "2025-12-01T02:28:26.792534Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNLI libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install transformers if needed\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q transformers torch\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"NLI libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "zk90ujak49o",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:35:19.825092Z",
     "start_time": "2025-12-01T02:34:32.479841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BART-Large MNLI model...\n",
      "(This model is specifically trained on Multi-NLI for entailment/contradiction)\n",
      "Note: Using BART instead of DeBERTa for better compatibility\n",
      "\n",
      "âœ“ Model loaded on CUDA device\n",
      "âœ“ NLI model ready!\n",
      "  Model size: ~560M parameters\n",
      "  Perfect for zero-shot classification on unseen rules!\n"
     ]
    }
   ],
   "source": [
    "# Load Zero-Shot NLI model\n",
    "print(\"Loading BART-Large MNLI model...\")\n",
    "print(\"(This model is specifically trained on Multi-NLI for entailment/contradiction)\")\n",
    "print(\"Note: Using BART instead of DeBERTa for better compatibility\\n\")\n",
    "\n",
    "nli_model_name = \"facebook/bart-large-mnli\"\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name)\n",
    "\n",
    "# Move to MPS if available\n",
    "if torch.backends.mps.is_available():\n",
    "    nli_model = nli_model.to('mps')\n",
    "    print(\"âœ“ Model loaded on MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    nli_model = nli_model.to('cuda')\n",
    "    print(\"âœ“ Model loaded on CUDA device\")\n",
    "else:\n",
    "    print(\"âœ“ Model loaded on CPU\")\n",
    "\n",
    "print(\"âœ“ NLI model ready!\")\n",
    "print(f\"  Model size: ~560M parameters\")\n",
    "print(f\"  Perfect for zero-shot classification on unseen rules!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15bpp3jucne",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:35:25.353264Z",
     "start_time": "2025-12-01T02:35:25.347769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ NLI scoring function defined!\n"
     ]
    }
   ],
   "source": [
    "def get_nli_scores(premises, hypotheses, batch_size=16):\n",
    "    \"\"\"\n",
    "    Compute NLI scores for premise-hypothesis pairs.\n",
    "    \n",
    "    For rule violation detection:\n",
    "    - premise: \"This text violates the rule: {rule}\"\n",
    "    - hypothesis: \"{body}\"\n",
    "    \n",
    "    Returns:\n",
    "    - entailment score (higher = more likely a violation)\n",
    "    - contradiction score (higher = less likely a violation)\n",
    "    - neutral score\n",
    "    \"\"\"\n",
    "    device = nli_model.device\n",
    "    all_entailment_scores = []\n",
    "    all_contradiction_scores = []\n",
    "    all_neutral_scores = []\n",
    "    \n",
    "    print(f\"Processing {len(premises)} samples in batches of {batch_size}...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(premises), batch_size)):\n",
    "        batch_premises = premises[i:i+batch_size]\n",
    "        batch_hypotheses = hypotheses[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = nli_tokenizer(\n",
    "            batch_premises,\n",
    "            batch_hypotheses,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = nli_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Extract scores (DeBERTa outputs: [contradiction, neutral, entailment])\n",
    "        contradiction_scores = probs[:, 0].cpu().numpy()\n",
    "        neutral_scores = probs[:, 1].cpu().numpy()\n",
    "        entailment_scores = probs[:, 2].cpu().numpy()\n",
    "        \n",
    "        all_contradiction_scores.extend(contradiction_scores)\n",
    "        all_neutral_scores.extend(neutral_scores)\n",
    "        all_entailment_scores.extend(entailment_scores)\n",
    "    \n",
    "    return np.array(all_entailment_scores), np.array(all_contradiction_scores), np.array(all_neutral_scores)\n",
    "\n",
    "print(\"âœ“ NLI scoring function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a144sybi1p",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-01T02:35:27.581874Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPUTING NLI FEATURES FOR TRAINING SET\n",
      "================================================================================\n",
      "\n",
      "Strategy: Frame the problem as entailment\n",
      "- If body ENTAILS 'this violates {rule}' â†’ violation likely\n",
      "- If body CONTRADICTS 'this violates {rule}' â†’ violation unlikely\n",
      "\n",
      "Computing direct violation scores (body vs rule)...\n",
      "Processing 2029 samples in batches of 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:05<00:00, 24.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing similarity to positive examples...\n",
      "Processing 2029 samples in batches of 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:07<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing similarity to negative examples...\n",
      "Processing 2029 samples in batches of 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:07<00:00, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Training NLI features shape: (2029, 7)\n",
      "  Features: entailment, contradiction, neutral, pos_sim, pos_dissim, neg_sim, neg_dissim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPUTING NLI FEATURES FOR TRAINING SET\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStrategy: Frame the problem as entailment\")\n",
    "print(\"- If body ENTAILS 'this violates {rule}' â†’ violation likely\")\n",
    "print(\"- If body CONTRADICTS 'this violates {rule}' â†’ violation unlikely\\n\")\n",
    "\n",
    "# Create NLI prompts for training set\n",
    "train_premises_v1 = [f\"This text violates the rule: {rule}\" for rule in train[rule_col]]\n",
    "train_hypotheses = train[body_col].tolist()\n",
    "\n",
    "print(\"Computing direct violation scores (body vs rule)...\")\n",
    "train_ent_v1, train_contra_v1, train_neu_v1 = get_nli_scores(train_premises_v1, train_hypotheses, batch_size=16)\n",
    "\n",
    "# Also check entailment with positive/negative examples\n",
    "print(\"\\nComputing similarity to positive examples...\")\n",
    "train_premises_pos = [f\"This is an example that violates the rule: {train['positive_example_1'].iloc[i]}\" \n",
    "                      for i in range(len(train))]\n",
    "train_ent_pos, train_contra_pos, _ = get_nli_scores(train_premises_pos, train_hypotheses, batch_size=16)\n",
    "\n",
    "print(\"\\nComputing similarity to negative examples...\")\n",
    "train_premises_neg = [f\"This is an example that does NOT violate the rule: {train['negative_example_1'].iloc[i]}\" \n",
    "                      for i in range(len(train))]\n",
    "train_ent_neg, train_contra_neg, _ = get_nli_scores(train_premises_neg, train_hypotheses, batch_size=16)\n",
    "\n",
    "# Combine NLI features\n",
    "X_nli_train = np.column_stack([\n",
    "    train_ent_v1,          # Entailment with violation\n",
    "    train_contra_v1,       # Contradiction with violation  \n",
    "    train_neu_v1,          # Neutral\n",
    "    train_ent_pos,         # Similarity to positive examples\n",
    "    train_contra_pos,      # Dissimilarity to positive examples\n",
    "    train_ent_neg,         # Similarity to negative examples\n",
    "    train_contra_neg,      # Dissimilarity to negative examples\n",
    "])\n",
    "\n",
    "print(f\"\\nâœ“ Training NLI features shape: {X_nli_train.shape}\")\n",
    "print(f\"  Features: entailment, contradiction, neutral, pos_sim, pos_dissim, neg_sim, neg_dissim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "t7km7rtakn",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (539044655.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtest_premises_v1x` = [f\"This text violates the rule: {rule}\" for rule in test[rule_col]]\u001b[39m\n                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPUTING NLI FEATURES FOR TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(\"This includes 4 UNSEEN RULES (66.76% of test data)!\\n\")\n",
    "\n",
    "# Create NLI prompts for test set\n",
    "test_premises_v1x` = [f\"This text violates the rule: {rule}\" for rule in test[rule_col]]\n",
    "test_hypotheses = test[body_col].tolist()\n",
    "\n",
    "print(\"Computing direct violation scores (body vs rule)...\")\n",
    "test_ent_v1, test_contra_v1, test_neu_v1 = get_nli_scores(test_premises_v1, test_hypotheses, batch_size=16)\n",
    "\n",
    "print(\"\\nComputing similarity to positive examples...\")\n",
    "test_premises_pos = [f\"This is an example that violates the rule: {test['positive_example_1'].iloc[i]}\" \n",
    "                     for i in range(len(test))]\n",
    "test_ent_pos, test_contra_pos, _ = get_nli_scores(test_premises_pos, test_hypotheses, batch_size=16)\n",
    "\n",
    "print(\"\\nComputing similarity to negative examples...\")\n",
    "test_premises_neg = [f\"This is an example that does NOT violate the rule: {test['negative_example_1'].iloc[i]}\" \n",
    "                     for i in range(len(test))]\n",
    "test_ent_neg, test_contra_neg, _ = get_nli_scores(test_premises_neg, test_hypotheses, batch_size=16)\n",
    "\n",
    "# Combine NLI features\n",
    "X_nli_test = np.column_stack([\n",
    "    test_ent_v1,\n",
    "    test_contra_v1,\n",
    "    test_neu_v1,\n",
    "    test_ent_pos,\n",
    "    test_contra_pos,\n",
    "    test_ent_neg,\n",
    "    test_contra_neg,\n",
    "])\n",
    "\n",
    "print(f\"\\nâœ“ Test NLI features shape: {X_nli_test.shape}\")\n",
    "print(f\"  Features: entailment, contradiction, neutral, pos_sim, pos_dissim, neg_sim, neg_dissim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "wcbcuzbamxo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING MODELS WITH NLI FEATURES\n",
      "================================================================================\n",
      "\n",
      "[1] Training Logistic Regression with NLI features ONLY...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_nli_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m lr_nli = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m, class_weight=\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      8\u001b[39m lr_nli.fit(X_nli_train, y)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m test_probs_nli = lr_nli.predict_proba(\u001b[43mX_nli_test\u001b[49m)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     11\u001b[39m test_preds_nli = (test_probs_nli > \u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'X_nli_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING MODELS WITH NLI FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test 1: NLI features only\n",
    "print(\"\\n[1] Training Logistic Regression with NLI features ONLY...\")\n",
    "lr_nli = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "lr_nli.fit(X_nli_train, y)\n",
    "\n",
    "test_probs_nli = lr_nli.predict_proba(X_nli_test)[:, 1]\n",
    "test_preds_nli = (test_probs_nli > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "test_results_nli = test[[\"row_id\"]].copy()\n",
    "test_results_nli[\"prediction\"] = test_probs_nli\n",
    "test_results_nli = test_results_nli.merge(solution[[\"row_id\", \"rule_violation\"]], on=\"row_id\", how=\"left\")\n",
    "y_test_nli = test_results_nli[\"rule_violation\"].values\n",
    "\n",
    "print(\"\\nMetrics on test dataset (NLI features only):\")\n",
    "print(classification_report(y_test_nli, test_preds_nli))\n",
    "test_auc_nli = roc_auc_score(y_test_nli, test_probs_nli)\n",
    "test_acc_nli = (test_preds_nli == y_test_nli).mean()\n",
    "print(f\"AUC: {test_auc_nli:.4f}\")\n",
    "print(f\"Accuracy: {test_acc_nli:.4f}\")\n",
    "\n",
    "results['Logistic Regression (NLI only)'] = {\n",
    "    'test_auc': test_auc_nli,\n",
    "    'test_acc': test_acc_nli,\n",
    "    'model': lr_nli,\n",
    "    'test_probs': test_probs_nli\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iu3chsb5sy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Combine NLI features with original similarity features\n",
    "print(\"\\n[2] Training with COMBINED features (NLI + Original)...\")\n",
    "\n",
    "X_combined = np.hstack([X_nli_train, X])  # 7 NLI features + 12 original = 19 features\n",
    "X_test_combined = np.hstack([X_nli_test, X_test])\n",
    "\n",
    "print(f\"Combined feature shape: {X_combined.shape}\")\n",
    "\n",
    "lr_combined = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "lr_combined.fit(X_combined, y)\n",
    "\n",
    "test_probs_combined = lr_combined.predict_proba(X_test_combined)[:, 1]\n",
    "test_preds_combined = (test_probs_combined > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "test_results_combined = test[[\"row_id\"]].copy()\n",
    "test_results_combined[\"prediction\"] = test_probs_combined\n",
    "test_results_combined = test_results_combined.merge(solution[[\"row_id\", \"rule_violation\"]], on=\"row_id\", how=\"left\")\n",
    "y_test_combined = test_results_combined[\"rule_violation\"].values\n",
    "\n",
    "print(\"\\nMetrics on test dataset (NLI + Original features):\")\n",
    "print(classification_report(y_test_combined, test_preds_combined))\n",
    "test_auc_combined = roc_auc_score(y_test_combined, test_probs_combined)\n",
    "test_acc_combined = (test_preds_combined == y_test_combined).mean()\n",
    "print(f\"AUC: {test_auc_combined:.4f}\")\n",
    "print(f\"Accuracy: {test_acc_combined:.4f}\")\n",
    "\n",
    "results['Logistic Regression (NLI + Original)'] = {\n",
    "    'test_auc': test_auc_combined,\n",
    "    'test_acc': test_acc_combined,\n",
    "    'model': lr_combined,\n",
    "    'test_probs': test_probs_combined\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vtfois9zsda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON: All Models Including Zero-Shot NLI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by test AUC\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['test_auc'], reverse=True)\n",
    "\n",
    "print(f\"\\n{'Model':<40} {'Test AUC':<12} {'Test Acc':<12} {'Improvement':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "best_previous_auc = max([res['test_auc'] for name, res in results.items() \n",
    "                         if 'NLI' not in name])\n",
    "\n",
    "for name, res in sorted_results:\n",
    "    auc = res['test_auc']\n",
    "    acc = res['test_acc']\n",
    "    \n",
    "    if 'NLI' in name:\n",
    "        improvement = auc - best_previous_auc\n",
    "        improvement_str = f\"+{improvement:.4f}\" if improvement > 0 else f\"{improvement:.4f}\"\n",
    "        marker = \"ðŸŽ¯ NEW!\" if improvement > 0 else \"\"\n",
    "        print(f\"{name:<40} {auc:<12.4f} {acc:<12.4f} {improvement_str:<12} {marker}\")\n",
    "    else:\n",
    "        print(f\"{name:<40} {auc:<12.4f} {acc:<12.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Highlight the best model\n",
    "best_model_name = sorted_results[0][0]\n",
    "best_auc = sorted_results[0][1]['test_auc']\n",
    "print(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test AUC: {best_auc:.4f}\")\n",
    "\n",
    "if 'NLI' in best_model_name:\n",
    "    improvement = best_auc - best_previous_auc\n",
    "    print(f\"   Improvement over previous best: +{improvement:.4f} ({improvement/best_previous_auc*100:.2f}%)\")\n",
    "    print(f\"\\nâœ¨ Zero-Shot NLI successfully handles UNSEEN RULES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ortqjag37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best NLI model's predictions\n",
    "best_nli_probs = results[best_model_name]['test_probs']\n",
    "submission_nli = test[[\"row_id\"]].copy()\n",
    "submission_nli[\"prediction\"] = best_nli_probs\n",
    "submission_nli.to_csv(\"submission_nli.csv\", index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Saved submission_nli.csv using {best_model_name}\")\n",
    "print(f\"  This model should perform MUCH better on unseen rules!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
