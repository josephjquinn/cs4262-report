{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Minimal Transformer Fine-tuning \n",
    "\n",
    "## Configuration\n",
    "- Model: DeBERTa-v3-small (86M params)\n",
    "- Batch size: 2\n",
    "- Epochs: 2\n",
    "- Folds: 2\n",
    "- Custom training loop (no HuggingFace Trainer)\n",
    "\n",
    "This version is optimized to prevent kernel crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crash prevention environment variables set\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CRITICAL: Prevent Windows Kernel Crash\n",
    "# ============================================\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "print(\"‚úÖ Crash prevention environment variables set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Device: cpu\n",
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Transformers (NO Trainer!)\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "\n",
    "# Limit threads\n",
    "torch.set_num_threads(2)\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration:\n",
      "   Model: microsoft/deberta-v3-small\n",
      "   Batch size: 2\n",
      "   Epochs: 2\n",
      "   Folds: 2\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Minimal configuration to prevent crashes.\"\"\"\n",
    "    # Model\n",
    "    model_name: str = 'microsoft/deberta-v3-small'\n",
    "    max_length: int = 128\n",
    "    \n",
    "    # Training\n",
    "    epochs: int = 2\n",
    "    batch_size: int = 2\n",
    "    accumulation_steps: int = 4\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Data\n",
    "    n_folds: int = 2\n",
    "    \n",
    "    # Paths\n",
    "    data_dir: Path = Path('data')\n",
    "    output_dir: Path = Path('outputs')\n",
    "\n",
    "config = Config()\n",
    "config.output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   Model: {config.model_name}\")\n",
    "print(f\"   Batch size: {config.batch_size}\")\n",
    "print(f\"   Epochs: {config.epochs}\")\n",
    "print(f\"   Folds: {config.n_folds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Data loaded:\n",
      "   Train: 2,029\n",
      "   Test:  54,059\n",
      "\n",
      "   Class distribution:\n",
      "rule_violation\n",
      "1    1031\n",
      "0     998\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(config.data_dir / 'train.csv')\n",
    "test_df = pd.read_csv(config.data_dir / 'test.csv')\n",
    "\n",
    "print(f\"üìä Data loaded:\")\n",
    "print(f\"   Train: {len(train_df):,}\")\n",
    "print(f\"   Test:  {len(test_df):,}\")\n",
    "print(f\"\\n   Class distribution:\")\n",
    "print(train_df['rule_violation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"Minimal dataset for text classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, max_length: int, mode: str = 'train'):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Combine body and rule\n",
    "        text = f\"{row['body']} [SEP] {row['rule']}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        \n",
    "        if self.mode == 'train' and 'rule_violation' in self.data.columns:\n",
    "            item['labels'] = torch.tensor(row['rule_violation'], dtype=torch.float)\n",
    "        \n",
    "        return item\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model class defined\n"
     ]
    }
   ],
   "source": [
    "class SimpleTransformerClassifier(nn.Module):\n",
    "    \"\"\"Simple transformer classifier with custom head.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load transformer\n",
    "        config_tf = AutoConfig.from_pretrained(model_name)\n",
    "        self.transformer = AutoModel.from_pretrained(model_name, config=config_tf)\n",
    "        \n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Simple classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get transformer output\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use CLS token\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled).squeeze(-1)\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "print(\"‚úÖ Model class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom Trainer defined\n"
     ]
    }
   ],
   "source": [
    "class CustomTrainer:\n",
    "    \"\"\"Custom training loop - NO HuggingFace Trainer.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, config, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        num_training_steps = len(train_loader) * config.epochs // config.accumulation_steps\n",
    "        num_warmup_steps = int(num_training_steps * config.warmup_ratio)\n",
    "        self.scheduler = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        \n",
    "        # Mixed precision\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "        # Tracking\n",
    "        self.best_auc = 0.0\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in enumerate(tqdm(self.train_loader, desc='Training', leave=False)):\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs['loss'] / self.config.accumulation_steps\n",
    "            \n",
    "            # Backward\n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            # Update weights every N steps\n",
    "            if (step + 1) % self.config.accumulation_steps == 0:\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * self.config.accumulation_steps\n",
    "        \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        \"\"\"Validate model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch in tqdm(self.val_loader, desc='Validating', leave=False):\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = self.model(**batch)\n",
    "            loss = outputs['loss']\n",
    "            logits = outputs['logits']\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_preds.extend(probs)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "        \n",
    "        return avg_loss, auc\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"Train for all epochs.\"\"\"\n",
    "        print(f\"\\nüöÄ Starting training...\\n\")\n",
    "        \n",
    "        for epoch in range(self.config.epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{self.config.epochs}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_auc = self.validate()\n",
    "            \n",
    "            # Save history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_auc'].append(val_auc)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss:   {val_loss:.4f}\")\n",
    "            print(f\"Val AUC:    {val_auc:.4f}\")\n",
    "            \n",
    "            # Save best\n",
    "            if val_auc > self.best_auc:\n",
    "                self.best_auc = val_auc\n",
    "                torch.save(self.model.state_dict(), 'best_model.pt')\n",
    "                print(f\"üíæ Best model saved! (AUC: {val_auc:.4f})\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        print(f\"‚úÖ Training complete! Best AUC: {self.best_auc:.4f}\")\n",
    "        return self.history\n",
    "\n",
    "print(\"‚úÖ Custom Trainer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training function defined\n"
     ]
    }
   ],
   "source": [
    "def train_with_cv(train_df, config, n_folds=2):\n",
    "    \"\"\"Train with k-fold cross-validation.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with {n_folds}-Fold CV\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    \n",
    "    # K-fold\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    fold_results = []\n",
    "    oof_predictions = np.zeros(len(train_df))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['rule_violation'])):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Split data\n",
    "        fold_train = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "        fold_val = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Train: {len(fold_train):,} | Val: {len(fold_val):,}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = SimpleDataset(fold_train, tokenizer, config.max_length, mode='train')\n",
    "        val_dataset = SimpleDataset(fold_val, tokenizer, config.max_length, mode='train')\n",
    "        \n",
    "        # Create loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # 0 to prevent crashes\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config.batch_size * 2,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = SimpleTransformerClassifier(config.model_name)\n",
    "        \n",
    "        # Train\n",
    "        trainer = CustomTrainer(model, train_loader, val_loader, config, device)\n",
    "        history = trainer.fit()\n",
    "        \n",
    "        # Get OOF predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = []\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "                probs = torch.sigmoid(outputs['logits']).cpu().numpy()\n",
    "                val_preds.extend(probs)\n",
    "        \n",
    "        oof_predictions[val_idx] = val_preds\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'best_auc': trainer.best_auc,\n",
    "            'history': history\n",
    "        })\n",
    "        \n",
    "        # Clean up\n",
    "        del model, trainer, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate overall CV score\n",
    "    cv_auc = roc_auc_score(train_df['rule_violation'], oof_predictions)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Cross-Validation Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for result in fold_results:\n",
    "        print(f\"Fold {result['fold']}: AUC = {result['best_auc']:.4f}\")\n",
    "    \n",
    "    avg_auc = np.mean([r['best_auc'] for r in fold_results])\n",
    "    print(f\"\\nMean AUC: {avg_auc:.4f}\")\n",
    "    print(f\"OOF AUC:  {cv_auc:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'oof_predictions': oof_predictions,\n",
    "        'cv_auc': cv_auc\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training with 2-Fold CV\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "results = train_with_cv(train_df, config, n_folds=config.n_folds)\n",
    "\n",
    "print(f\"\\nüèÜ Final CV AUC: {results['cv_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_test_predictions(test_df, config):\n",
    "    \"\"\"Generate predictions on test set.\"\"\"\n",
    "    \n",
    "    print(\"\\nüîÆ Generating test predictions...\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    model = SimpleTransformerClassifier(config.model_name)\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dataset\n",
    "    test_dataset = SimpleDataset(test_df, tokenizer, config.max_length, mode='test')\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    predictions = []\n",
    "    for batch in tqdm(test_loader, desc='Predicting'):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        probs = torch.sigmoid(outputs['logits']).cpu().numpy()\n",
    "        predictions.extend(probs)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Generate predictions\n",
    "test_predictions = generate_test_predictions(test_df, config)\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions generated!\")\n",
    "print(f\"   Min: {test_predictions.min():.4f}\")\n",
    "print(f\"   Max: {test_predictions.max():.4f}\")\n",
    "print(f\"   Mean: {test_predictions.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'rule_violation': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(config.output_dir / 'submission.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Submission saved to outputs/submission.csv\")\n",
    "print(f\"\\nüìä Submission preview:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(f\"\\nüéâ ALL DONE!\")\n",
    "print(f\"   CV AUC: {results['cv_auc']:.4f}\")\n",
    "print(f\"   Model: {config.model_name}\")\n",
    "print(f\"   File: outputs/submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
